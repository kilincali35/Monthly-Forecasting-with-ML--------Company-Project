{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This forecast model uses the input table created with the \"input transformer code\" applied in another Jupyter notebook. This notebook is about monthly forecasting of sales data. For training and model creation; I used most of the historical data as training data and selected the last N months as test set; N is specified at the beginning of code as a parameter.\n",
    "\n",
    "Test set is consisting of 0 values for test months at beginning. Think about N rows to be predicted. When 1st month of the test set is predicted, it is also written on another rows in a backwards shifting pattern, according to \"Last M months\" parameter set at the beginning. For example, when January row is predicted, it is written into N-1 column at February row and N-2 column at March row etc. Then February prediction is conducted and written into proper columns at next rows. \n",
    "\n",
    "After I get the predictions of model, I compared them to the real life predictions made by sales department for these last N months.\n",
    "\n",
    "In the first version of model, I used all of the data in same pool. Most of the combinations have average sales between 0-10; on the other hand a little portion of them are above 100. So, i decided to divide dataset into 3 segments; 0-10 / 10-100 /100+; and trained different models for each of the segments. \n",
    "\n",
    "Model structure is based on a classification stage at first. Dataset is a \"0 dense dataset\" where about 50% of the data contains 0 as a sales output. This creates some issues for a regression problem. 0 values leverages the predictions towards 0.\n",
    "\n",
    "When I divided the problem into 2 stages; I also could neglect 0 values from regression problem. In first stage, classification algorithm decides if a values is 0 or \"not 0\". If it predicts the value as 0, then a regression algorithm will not be applied for that row and prediction is written a 0. But if it is labeled as \"not 0\", second stage regression algorithm will be applied for that row and regression result will be written as a prediction.\n",
    "\n",
    "Classification dataset consists of all the input data (if it is balanced - daily prediction dataset was imbalanced and I applied (80% 0 20% else) an undersampling algorithm to 0 values to make it balanced, I am adding it into this code also as a comment-) whereas regression data neglects 0 values from the input to eliminate their leverage effect.\n",
    "\n",
    "Inside this development code there are several parameters for experimental trials like nestedcv parameter, or neural network parameter etc. They are set at the beginning according to the experimental design and model is run according to these settings. Mostly, I am using hyperparameter search on several algorithms to get a better performance from this dataset. And also I use cross validation to get a better estimate of errors from different runs.\n",
    "\n",
    "After getting best of the best model among all experiments, a shortened version of this code can be used for further production stages with the best hyperparameters for the selected model, making predictions on newcoming data.\n",
    "\n",
    "This project is simply for ad-hoc usage. It will be used at a monthly frequency to update monthly forecasts for future sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns',40)\n",
    "import more_itertools as mi\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning, ConvergenceWarning\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "np.random.seed(234)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#part that writes model parameter outputs into a text file\n",
    "text_file = open(r'C:\\Users\\ali.kilinc\\Desktop\\Tahminleme\\Output.txt',\"w\")\n",
    "\n",
    "#part that ignores some warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "start_time = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali.kilinc\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "forecast = pd.read_csv(r'C:\\Users\\ali.kilinc\\Desktop\\Tahminleme\\12M Yeni Versiyon\\12M2020FInputAllNew.csv', index_col= False)\n",
    "\n",
    "param = 9 # last M months parameter to be included as input variable\n",
    "predmonths = 12 # N months to be predicted in a moving pattern\n",
    "diff = 3 #get last X month's change ratios as input\n",
    "subsets = 1 # it is 1 if it is a segmented version\n",
    "transform = 1 #If transformation will be applied to the y values or not\n",
    "pre_classify = 1 #Will pre-classification stage will be applied to the dataset?\n",
    "regress = 1 # will regression stage will be applied\n",
    "hypertest = 1 #Hyperparameter search option is on or off \n",
    "nestedcv = 0 # nestedcv option is on or off\n",
    "neural_model = 0 # will a neural model be applied on dataset?\n",
    "neural_model_final = 0 #If a neural model is trained, after writing the best parameters run a final training on whole dataset\n",
    "finalize = 0 # if a classical ML model is trained, after writing the best parameters, run a final training on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part that segments are listed\n",
    "seg_list = list(mi.unique_everseen(forecast['segment']))\n",
    "\n",
    "#if it is not a segmented model, related parts will be dropped from table\n",
    "if subsets == 0:\n",
    "    forecast.drop(columns = ['segment'], axis = 1, inplace = True)\n",
    "else:\n",
    "    forecast['segment'] = forecast['segment'].astype('category')\n",
    "\n",
    "forecast['code'] = forecast['code'].astype('str')\n",
    "\n",
    "#forecast = forecast[forecast['segment'] != 3]\n",
    "\n",
    "#CSV document erases the leading 0's from company code, this code fills them\n",
    "forecast['code'][forecast['code'].str[:1].str.isnumeric() == True] = forecast['code'][forecast['code'].str[:1].str.isnumeric() == True].str.zfill(10)\n",
    "\n",
    "# extracting binary column from a numeric column with a condition\n",
    "forecast['yclass'] = (forecast['M'] > 0).astype(int)\n",
    "#forecast['yclass'] = np.where(forecast['M'] > 0, 1, 0)\n",
    "#forecast['yclass'] = forecast['M'].apply(lambda x: 1 if x > 0 else 0) it is slower than others\n",
    "\n",
    "#transforming date variables\n",
    "forecast['date'] = pd.to_datetime(forecast['date'], format = '%Y-%m')\n",
    "forecast['date'] = forecast['date'].dt.to_period('m')\n",
    "\n",
    "#reserving to-be-predicted months as pred_set. Also setting some rows as neglected according to some work rules\n",
    "pred_set = forecast[(forecast['date'] <= forecast['date'].max()) & (forecast['date'] > forecast['date'].max() - (predmonths+1))]\n",
    "forecast = forecast[forecast['date'] <= forecast['date'].max() - (predmonths+1)]\n",
    "\n",
    "dropout = forecast[(((forecast['code'] + forecast['service'] + forecast['from'] + forecast['to']).isin(['0000052275İhracatSIDE', '0000052275İthalatDESI']) == True)\n",
    "                   & (forecast['moving3'] == 0)) | (forecast['year'] < 2017)].index\n",
    "\n",
    "                  \n",
    "forecast.drop(index = dropout, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_drop = []\n",
    "final_drop = []\n",
    "\n",
    "for col in forecast.columns:\n",
    "    \n",
    "    if ('M-' in col):\n",
    "        \n",
    "        if (int(re.findall('\\d+', col)[0]) > param) & (int(re.findall('\\d+', col)[0]) > predmonths):\n",
    "            forecast.drop(col, axis = 1, inplace = True)\n",
    "            pred_set.drop(col, axis = 1, inplace = True)\n",
    "            \n",
    "        elif (int(re.findall('\\d+', col)[0]) > param) & (int(re.findall('\\d+', col)[0]) <= predmonths):\n",
    "            forecast.drop(col, axis = 1, inplace = True)\n",
    "            pred_drop.append(col)\n",
    "            \n",
    "        elif (int(re.findall('\\d+', col)[0]) <= param) & (int(re.findall('\\d+', col)[0]) > predmonths):\n",
    "            final_drop.append(col)\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code dynamically sets Past Month input values on training set and test set. Some of them are totally not needed and dropped directly. Some of them are inserted into a list of columns to be dropped while inserting prediction values to test set at the end.\n",
    "\n",
    "Below code drops some unwanted features from the model. Also plotting part is inactive in this version, they were used for the analysis of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code', 'service', 'from', 'to', 'date', 'M-12', 'M-11', 'M-10', 'M-9',\n",
      "       'M-8', 'M-7', 'M-6', 'M-5', 'M-4', 'M-3', 'M-2', 'M-1', 'M', 'segment',\n",
      "       'month', 'quarter', 'year', 'quarterind', 'monthind', 'moving3',\n",
      "       'moving6', 'moving12', 'Dolar', 'Euro', 'dptholidaycnt', 'dptmoncnt',\n",
      "       'dpttuecnt', 'dptwedcnt', 'dptthurscnt', 'dptfridaycnt',\n",
      "       'arvholidaycnt', 'arvmoncnt', 'arvtuecnt', 'arvwedcnt', 'arvthurscnt',\n",
      "       'arvfridaycnt', 'D-1', 'D-2', 'D-3', 'pdptholiday', 'parvholiday',\n",
      "       'pyearsales', 'yclass'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfig, ax = plt.subplots(2,2)\\n\\nax[0,0] = forecast['dptholidaycnt'].value_counts().plot(kind = 'bar', ax = ax[0,0])\\n\\nax[0,1] = forecast['arvholidaycnt'].value_counts().plot(kind = 'bar', ax = ax[0,1])\\n\\nax[1,0] = forecast['dptfridaycnt'].value_counts().plot(kind = 'bar', ax = ax[1,0])\\n\\nax[1,1] = forecast['arvfridaycnt'].value_counts().plot(kind = 'bar', ax = ax[1,1])\\n\\n\\n\\nfig2, ax2 = plt.subplots(1,2)\\n\\nax2[0].boxplot(forecast['Dolar'])\\n\\nax2[1].boxplot(forecast['Euro'])\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast.drop(columns= ['date', 'year', 'quarterind', 'monthind'], inplace = True)\n",
    "print(pred_set.columns)\n",
    "pred_unique = pred_set['date']\n",
    "pred_set.drop(columns= ['date', 'year', 'quarterind', 'monthind'], inplace = True)\n",
    "\n",
    "\"\"\" related to first version, but as a library it can stay\n",
    "for i in range(0,param):\n",
    "    if i < param - 1:\n",
    "        pred_set[str('M-'+str(param - i))] = pred_set[str('M-'+str(param - i - 1))]\n",
    "    else:\n",
    "        pred_set['M-1'] = pred_set['M']\n",
    "\"\"\"\n",
    "\n",
    "pred_set.drop(columns= ['M'], inplace = True)\n",
    "\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(2,2)\n",
    "\n",
    "ax[0,0] = forecast['dptholidaycnt'].value_counts().plot(kind = 'bar', ax = ax[0,0])\n",
    "\n",
    "ax[0,1] = forecast['arvholidaycnt'].value_counts().plot(kind = 'bar', ax = ax[0,1])\n",
    "\n",
    "ax[1,0] = forecast['dptfridaycnt'].value_counts().plot(kind = 'bar', ax = ax[1,0])\n",
    "\n",
    "ax[1,1] = forecast['arvfridaycnt'].value_counts().plot(kind = 'bar', ax = ax[1,1])\n",
    "\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(1,2)\n",
    "\n",
    "ax2[0].boxplot(forecast['Dolar'])\n",
    "\n",
    "ax2[1].boxplot(forecast['Euro'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast['comb'] = (forecast['code'] + forecast['service'] + forecast['from'] + forecast['to'])\n",
    "pred_set['comb'] = (pred_set['code'] + pred_set['service'] + pred_set['from'] + pred_set['to'])\n",
    "\n",
    "#creating variables for moving averages\n",
    "\n",
    "movingdict = dict()\n",
    "\n",
    "for w in [12]:\n",
    "#!!!!for w in [3,6,12]:\n",
    "    movinglist = []\n",
    "    for col in pred_set.columns:\n",
    "        if ('M-' in col):\n",
    "            if (int(re.findall('\\d+', col)[0])) <= w:\n",
    "                movinglist.append(col)\n",
    "    movingdict[w] = movinglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#this code part uses only comb column, transforms a comb name to other if it is above threshold according to 0 values among timespan\\n# this coluld be done with the code line at table preparation code. This would not delete the line but change the combination at beginning\\n# same can be applied to from and to seperately. \\nforecast.drop(columns = ['from', 'to'], axis=1, inplace = True)\\n\\nthreshold = 0.9\\n\\ncomblist = sorted(set(forecast['comb']))\\ncountlist = []\\nfor item in comblist:\\n    #subdf = forecast[['comb', str('M-' + str(param)), 'M']][forecast['comb'] == item]\\n    subdf1 = forecast[['comb', str('M-' + str(param))]][forecast['comb'] == item]\\n    subdf2 = forecast[['comb', 'M']][forecast['comb'] == item]\\n    subdf2.columns = ['comb', str('M-' + str(param))]\\n    #subdf[['comb', str('M-' + str(param))]] = subdf[['comb', str('M-' + str(param))]].append(subdf[['comb','M']].iloc[-param:], ignore_index = True)\\n    subdf1 = subdf1.append(subdf2.iloc[-param:], ignore_index = True)\\n    templist = [item, len(subdf1[subdf1[str('M-' + str(param))] == 0])/len(subdf1)]\\n    countlist.append(templist)\\n    templist = []\\n\\ncountdf = pd.DataFrame(countlist, columns = ['comb', 'zerocnt'])\\n#print(list(countdf['comb'][countdf['zerocnt'] > threshold]))\\n\\ncountdf['zerocnt'].plot(kind = 'hist')\\n#print(countdf['comb'].value_counts())\\n\\nforecast['comb'].replace(list(countdf['comb'][countdf['zerocnt'] > threshold]) , 'oth', inplace = True)\\n#print(forecast['comb'].value_counts().head(170))\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for some ad-hoc visualizations. I used them for analysis and deactivated then\n",
    "\n",
    "'''\n",
    "#I tried to make a dynamic figure which I can zoom in at the specific areas but I couldn't right now. After saving into computer, it was possible. \n",
    "#So i decided to divide all graphs to multiple subplots with a dynamic formula. It was more interpretable. \n",
    "\n",
    "figcnt = ceil(len(sorted(set(forecast['comb'])))/35)\n",
    "plotcnt = 0\n",
    "\n",
    "for m in range(figcnt):\n",
    "    fig = str('fig'+'_'+str(figcnt))\n",
    "    ax = str('ax'+'_'+str(figcnt))\n",
    "    fig, ax = plt.subplots(7,5)\n",
    "    plt.subplots_adjust(left = 0.03, right = 0.95, top = 0.95, bottom = 0.05)\n",
    "    for i ,j in itertools.product(range(7), range(5)):\n",
    "        if plotcnt < len(sorted(set(forecast['comb']))):\n",
    "            ax[i,j] = forecast[['comb', 'M']][forecast['comb']==sorted(set(forecast['comb']))[plotcnt]].plot(kind = 'hist', ax = ax[i,j])\n",
    "            ax[i,j].set(title = sorted(set(forecast['comb']))[plotcnt])\n",
    "            plotcnt += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "'''\n",
    "'''\n",
    "#this code part uses only comb column, transforms a comb name to other if it is above threshold according to 0 values among timespan\n",
    "# this coluld be done with the code line at table preparation code. This would not delete the line but change the combination at beginning\n",
    "# same can be applied to from and to seperately. \n",
    "forecast.drop(columns = ['from', 'to'], axis=1, inplace = True)\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "comblist = sorted(set(forecast['comb']))\n",
    "countlist = []\n",
    "for item in comblist:\n",
    "    #subdf = forecast[['comb', str('M-' + str(param)), 'M']][forecast['comb'] == item]\n",
    "    subdf1 = forecast[['comb', str('M-' + str(param))]][forecast['comb'] == item]\n",
    "    subdf2 = forecast[['comb', 'M']][forecast['comb'] == item]\n",
    "    subdf2.columns = ['comb', str('M-' + str(param))]\n",
    "    #subdf[['comb', str('M-' + str(param))]] = subdf[['comb', str('M-' + str(param))]].append(subdf[['comb','M']].iloc[-param:], ignore_index = True)\n",
    "    subdf1 = subdf1.append(subdf2.iloc[-param:], ignore_index = True)\n",
    "    templist = [item, len(subdf1[subdf1[str('M-' + str(param))] == 0])/len(subdf1)]\n",
    "    countlist.append(templist)\n",
    "    templist = []\n",
    "\n",
    "countdf = pd.DataFrame(countlist, columns = ['comb', 'zerocnt'])\n",
    "#print(list(countdf['comb'][countdf['zerocnt'] > threshold]))\n",
    "\n",
    "countdf['zerocnt'].plot(kind = 'hist')\n",
    "#print(countdf['comb'].value_counts())\n",
    "\n",
    "forecast['comb'].replace(list(countdf['comb'][countdf['zerocnt'] > threshold]) , 'oth', inplace = True)\n",
    "#print(forecast['comb'].value_counts().head(170))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining input and target variables. Also creating different tables for classification and regression\n",
    "df_y = forecast['M']\n",
    "df_y_class = forecast['yclass']\n",
    "df_x = forecast.drop(columns = ['M', 'yclass'], axis=1)\n",
    "pred_set = pred_set.drop(columns = ['yclass'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables like quarter, month, weekday are actually cyclic variables besides being categorical variables. Below I made experiments for these variables, either being cyclic or categorical. At the end, model performance was better when they were treated as categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#!!!!!!!! IT IS THE TRANSFORMATION OF CYCLICAL VARIABLES LIKE MONTH AND QUARTER.\n",
    "df_x['month_x'] = np.sin(2*np.pi*df_x['month']/12)\n",
    "df_x['month_y'] = np.cos(2*np.pi*df_x['month']/12)\n",
    "\n",
    "df_x['quarter_x'] = np.sin(2*np.pi*df_x['quarter']/4)\n",
    "df_x['quarter_y'] = np.sin(2-np.pi*df_x['quarter']/4)\n",
    "\n",
    "pred_set['month_x'] = np.sin(2*np.pi*pred_set['month']/12)\n",
    "pred_set['month_y'] = np.cos(2*np.pi*pred_set['month']/12)\n",
    "\n",
    "pred_set['quarter_x'] = np.sin(2*np.pi*pred_set['quarter']/4)\n",
    "pred_set['quarter_y'] = np.sin(2-np.pi*pred_set['quarter']/4)\n",
    "\n",
    "df_x.drop(columns = ['month', 'quarter'], axis = 1, inplace = True)\n",
    "pred_set.drop(columns = ['month', 'quarter'], axis = 1, inplace = True)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# General function for this transformation\n",
    "def encode(data, col, max_val): # max val is theoretical maximum of column, for month it is 12, for day of year it is 365 etc.\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data\n",
    "'''\n",
    "\n",
    "df_x['month'] = df_x['month'].astype('category')\n",
    "df_x['quarter'] = df_x['quarter'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'service', 'to', 'segment', 'comb', 'quarter', 'code', 'month']\n",
      "['M-3', 'moving6', 'M-6', 'M-9', 'dptmoncnt', 'arvwedcnt', 'M-7', 'arvmoncnt', 'moving3', 'arvholidaycnt', 'arvtuecnt', 'D-2', 'dptwedcnt', 'M-8', 'M-2', 'dptfridaycnt', 'M-1', 'M-4', 'arvthurscnt', 'D-3', 'dptholidaycnt', 'Dolar', 'Euro', 'dptthurscnt', 'dpttuecnt', 'pyearsales', 'M-5', 'D-1', 'pdptholiday', 'moving12', 'parvholiday', 'arvfridaycnt']\n"
     ]
    }
   ],
   "source": [
    "#Creating lists according to column types.Eliminating fixed or empty ones. Dividing numeric and categoric columns\n",
    "num_cols = list(df_x._get_numeric_data().columns)\n",
    "\n",
    "fix_cols = []\n",
    "for col in df_x.columns:\n",
    "    if df_x[col].nunique() == 1:\n",
    "        fix_cols.append(col)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "emp_cols = []\n",
    "for col in df_x.columns:\n",
    "    if df_x[col].count() > 0:\n",
    "        pass\n",
    "    else:\n",
    "        emp_cols.append(col)\n",
    "        \n",
    "cat_cols = list(set(df_x.columns) - set(num_cols) - set(emp_cols) - set(fix_cols))\n",
    "num_cols = list(set(num_cols) - set(fix_cols) - set(emp_cols))\n",
    "\n",
    "df_x.drop(columns = list(emp_cols+fix_cols), inplace = True)\n",
    "pred_set.drop(columns = list(emp_cols+fix_cols), inplace = True) #!!!!!!!!!!!!!!!!!\n",
    "\n",
    "print(cat_cols)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#On daily dataset, proportion of 0 values to whole dataset is about 75% which means an imbalance on classification dataset\\n#For algorithm, it is much easier to label most of the data as 0, and this means sacrificing from quality.\\n#Because of that reason, some of the 0 values are removed from dataset with RandomUnderSampling.\\nrus = RandomUnderSampler(random_state = 23, sampling_strategy = 1)\\narr_x, arr_y_class = rus.fit_resample(df_x.to_numpy(), df_y_class.to_numpy())\\n\\ndf_x = pd.DataFrame(arr_x, columns = df_x.columns)\\ndf_y_class = pd.DataFrame(arr_y_class, columns = ['yclass'])\\ndf_x[['quarter', 'month', 'weekday']] = df_x[['quarter', 'month', 'weekday']].astype('category')\\ndf_x[num_cols] = df_x[num_cols].astype('float64')\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = pd.DataFrame(df_x, columns = df_x.columns)\n",
    "df_y = pd.DataFrame(df_y, columns = ['M'])\n",
    "df_y_class = pd.DataFrame(df_y_class, columns = ['yclass'])\n",
    "\n",
    "#df_x = df_x.loc[df_x[df_x['segment']==3].index]\n",
    "#df_y_class = df_y_class.loc[df_x[df_x['segment']==3].index]\n",
    "#pred_set = pred_set.loc[pred_set[pred_set['segment']==3].index]\n",
    "\n",
    "#shuffling the data before used in training; it is important to shuffle x and y with same indexes to protect ordering\n",
    "for i in range(100):\n",
    "    idx = np.random.permutation(df_x.index)\n",
    "    df_x = df_x.reindex(idx)\n",
    "    df_y = df_y.reindex(idx)\n",
    "    df_y_class = df_y_class.reindex(idx)\n",
    "    \n",
    "#if class value is 1, rows are saved as another table to be used in regression analysis. So, 0 values are eliminated from regression at first part\n",
    "df_x_regr = df_x.loc[df_y_class[df_y_class['yclass']==1].index]\n",
    "df_y_regr = df_y.loc[df_y_class[df_y_class['yclass']==1].index]\n",
    "\n",
    "'''\n",
    "#On daily dataset, proportion of 0 values to whole dataset is about 75% which means an imbalance on classification dataset\n",
    "#For algorithm, it is much easier to label most of the data as 0, and this means sacrificing from quality.\n",
    "#Because of that reason, some of the 0 values are removed from dataset with RandomUnderSampling.\n",
    "rus = RandomUnderSampler(random_state = 23, sampling_strategy = 1)\n",
    "arr_x, arr_y_class = rus.fit_resample(df_x.to_numpy(), df_y_class.to_numpy())\n",
    "\n",
    "df_x = pd.DataFrame(arr_x, columns = df_x.columns)\n",
    "df_y_class = pd.DataFrame(arr_y_class, columns = ['yclass'])\n",
    "df_x[['quarter', 'month', 'weekday']] = df_x[['quarter', 'month', 'weekday']].astype('category')\n",
    "df_x[num_cols] = df_x[num_cols].astype('float64')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M-9', 'M-8', 'M-7', 'M-6', 'M-5', 'M-4', 'M-3', 'M-2', 'M-1']\n"
     ]
    }
   ],
   "source": [
    "month_var = []\n",
    "\n",
    "for i in range(0,param):\n",
    "    month_var.append(str('M-'+str(param -i)))\n",
    "\n",
    "print(month_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code', 'service', 'from', 'to', 'M-12', 'M-11', 'M-10', 'M-9', 'M-8',\n",
      "       'M-7', 'M-6', 'M-5', 'M-4', 'M-3', 'M-2', 'M-1', 'segment', 'month',\n",
      "       'quarter', 'moving3', 'moving6', 'moving12', 'Dolar', 'Euro',\n",
      "       'dptholidaycnt', 'dptmoncnt', 'dpttuecnt', 'dptwedcnt', 'dptthurscnt',\n",
      "       'dptfridaycnt', 'arvholidaycnt', 'arvmoncnt', 'arvtuecnt', 'arvwedcnt',\n",
      "       'arvthurscnt', 'arvfridaycnt', 'D-1', 'D-2', 'D-3', 'pdptholiday',\n",
      "       'parvholiday', 'pyearsales', 'comb'],\n",
      "      dtype='object')\n",
      "1    24183\n",
      "0    14761\n",
      "Name: yclass, dtype: int64\n",
      "yclass\n",
      "0    14761\n",
      "1    24183\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#y is a lognormal distribution with mostly 0 values, i will try to fit a log(x+1) transformation for that.( same for all month variables)\n",
    "#log1p(x) is equal to log(x+1). reverse of it is expm1(y)\n",
    "#log transformation, or similars, can be applied to whole data as it has no data dependent variable that can lead to a data leakage!!!\n",
    "#Transform input features if needed, but it is not an assumption of regression(normality of ind variables)\n",
    "#highly skewed variables can be normalized with transformations to increase predictive power!!!!!\n",
    "##https://www.researchgate.net/post/Should_I_transform_non-normal_independent_variables_in_logistic_regression/564f3cb65e9d97d2a58b457d/citation/download\n",
    "\n",
    "if transform == 1:\n",
    "    #stats.probplot(df_y_regr.values.ravel(),plot=plt)\n",
    "    #sns.distplot(df_y_regr, hist = False, kde = True)\n",
    "    #plt.show()\n",
    "    #df_y_regr = np.log1p(df_y_regr + 10) # iyi sonuç\n",
    "    #df_y_regr = np.log1p(df_y_regr + 10000)\n",
    "    df_y_regr = np.log1p(df_y_regr)\n",
    "    #df_y_regr = pow(df_y_regr, 0.125)\n",
    "    #stats.probplot(df_y_regr.values.ravel(),plot=plt)\n",
    "    #sns.distplot(df_y_regr, hist = False, kde = True)\n",
    "    #plt.show()\n",
    "    #df_x[month_var] = np.log(df_x[month_var]+1)\n",
    "    #pred_set[month_var] = np.log(pred_set[month_var]+1)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print(pred_set.columns)\n",
    "print(df_y_class['yclass'].value_counts())\n",
    "print(df_y_class.groupby(['yclass']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted', 'max_error', 'mutual_info_score', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_gamma_deviance', 'neg_mean_poisson_deviance', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'roc_auc_ovo', 'roc_auc_ovo_weighted', 'roc_auc_ovr', 'roc_auc_ovr_weighted', 'v_measure_score']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, cohen_kappa_score\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn.metrics as m\n",
    "from lightgbm import LGBMClassifier\n",
    "import category_encoders as ce\n",
    "\n",
    "print(sorted(m.SCORERS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_classify == 1:\n",
    "    \n",
    "    modelsc = dict()\n",
    "    \n",
    "    paramsrfc = {\n",
    "            'max_depth':[5, 9, 18, 32],\n",
    "            'n_estimators':[10, 50, 100, 200],\n",
    "            'min_samples_split': np.linspace(0.1, 1.0, 3, endpoint=True),\n",
    "            'min_samples_leaf':np.linspace(0.1, 0.5, 3, endpoint=True)\n",
    "            }\n",
    "    \n",
    "    paramslgbc = {\n",
    "            'max_depth':[5, 9, 18, 32],\n",
    "            'learning_rate': [0.001, 0.01, 0.1],\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': np.linspace(11,51,3,endpoint = True, dtype = int)\n",
    "            }\n",
    "    \n",
    "    paramsknnc = {'n_neighbors':[6,12,20]}\n",
    "    \n",
    "    paramssgdc = {\n",
    "            'penalty':['l1','l2','elasticnet'],\n",
    "            'max_iter': np.linspace(100,700,3,endpoint = True),\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'n_iter_no_change':np.linspace(5, 17, 3, endpoint=True)\n",
    "            }\n",
    "    \n",
    "    paramsxgbc = {\n",
    "            'n_estimator': np.linspace(100,500,2,endpoint = True),\n",
    "            'max_depth': np.linspace(3,15,3, endpoint=True, dtype = int),\n",
    "            #'min_child_weight':np.linspace(1, 9, 3, endpoint=True),\n",
    "            #'colsample_bytree': np.linspace(0.3,0.5,3,endpoint =True),\n",
    "            'learning_rate': [0.001, 0.01, 0.1]\n",
    "            }\n",
    "    \n",
    "    paramslogitc = {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l2'],\n",
    "            'tol': [0.0001, 0.001, 0.01]\n",
    "            }\n",
    "    \n",
    "    #random forest-logit has given \"AttributeError, none object has no module named write\" error with njobs = -1 for gridsearch and verbose = 1 for Random Forest. I closed verbose\n",
    "    #modelsc['rf'] = [RandomForestClassifier(), paramsrfc]\n",
    "    modelsc['lgb'] = [LGBMClassifier(verbosity = 1), paramslgbc]\n",
    "    ##modelsc['knn'] = [KNeighborsClassifier(), paramsknnc]\n",
    "    #modelsc['sgd'] = [SGDClassifier(verbose = 1, loss = 'log'), paramssgdc] #saçma bir sonuç verdi, iyi koruna rağmen\n",
    "    #modelsc['gbm'] = [XGBClassifier(seed = 23, verbosity = 1), paramsxgbc]\n",
    "    #modelsc['logit'] = [LogisticRegression(solver = 'sag'), paramslogitc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   58.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 10.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_time = 2020-03-17 17:04:10.608827...\n"
     ]
    }
   ],
   "source": [
    "    for key, value in modelsc.items():\n",
    "        \n",
    "        start_timec = timeit.default_timer()\n",
    "                \n",
    "        scorer = ['roc_auc', 'accuracy', 'f1', 'jaccard', 'precision', 'recall']\n",
    "                    \n",
    "        numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (0,1)))\n",
    "        categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "        preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "        \n",
    "        all_pipe = make_pipeline(preprocessor, value[0])\n",
    "        \n",
    "        grid_search = GridSearchCV(all_pipe, value[1], cv=5, verbose=1, refit = 'roc_auc', scoring = scorer, return_train_score = True, n_jobs = -1)\n",
    "        #scoring option is for defining multiple scorers, otherwise null is OK\n",
    "        #refit must be chosen if multi scorers is selected. But best_score_, best_params_ etc will give only the result of that metric, cant get multi scores\n",
    "        #here REFIT option tells you which metric do you want to consider for best_params_ calculation(according to which metric)\n",
    "       \n",
    "        \n",
    "        grid_search.fit(df_x, df_y_class.values.ravel())\n",
    "        \n",
    "        #from the resulting parameters dictionary, we choose the index of best_param_ set. This index will help us the get best param value from subresults\n",
    "        #metric results are in arraf form, in each array there is a list of results corresponding to the all parameter combinations. \n",
    "        ind = grid_search.best_index_      \n",
    "        \n",
    "        print(\"model = {}\".format(key), file = text_file)\n",
    "        print(\"train_roc = {}, test_roc ={}\".format(grid_search.cv_results_['mean_train_roc_auc'][ind], grid_search.cv_results_['mean_test_roc_auc'][ind]), file = text_file)\n",
    "        print(\"train_acc = {}, test_acc ={}\".format(grid_search.cv_results_['mean_train_accuracy'][ind], grid_search.cv_results_['mean_test_accuracy'][ind]), file = text_file)\n",
    "        print(\"train_f1 = {}, test_f1 = {}\".format(grid_search.cv_results_['mean_train_f1'][ind], grid_search.cv_results_['mean_test_f1'][ind]), file = text_file)\n",
    "        print(\"train_jaccard = {}, test_jaccard = {}\".format(grid_search.cv_results_['mean_train_jaccard'][ind], grid_search.cv_results_['mean_test_jaccard'][ind]), file = text_file)\n",
    "        print(\"train_precision = {}, test_precision = {}\".format(grid_search.cv_results_['mean_train_precision'][ind], grid_search.cv_results_['mean_test_precision'][ind]), file = text_file)\n",
    "        print(\"train_recall = {}, test_recall = {}\".format(grid_search.cv_results_['mean_train_recall'][ind], grid_search.cv_results_['mean_test_recall'][ind]), file = text_file)\n",
    "        print(\"avg_fit_time = {}\".format(grid_search.cv_results_['mean_fit_time'][ind]), file = text_file)\n",
    "        #we are getting best_params_ from grid_search object still, it is valid\n",
    "        print(\"best_params = {}\".format(grid_search.best_params_), file = text_file)\n",
    "        print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_timec)/60), file = text_file)\n",
    "        print(\"run_start = {}...\".format(start_time), file = text_file)\n",
    "        print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "        print(\"current_time = {}...\".format(datetime.now()))\n",
    "        print(\"-----------------------------------\", file = text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code is a hyperparameter search code with a CV. All included models are run with hyperparameter search and at the end all results are written inside a text file as a summary. Written results are regarding the fold with best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Lasso, Ridge, ElasticNet, HuberRegressor, Lars, RANSACRegressor, PassiveAggressiveRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regress == 1:\n",
    "\n",
    "    models = dict()\n",
    "    \n",
    "    paramsgbm = {\n",
    "            'n_estimator': np.linspace(100,500,2,endpoint = True),\n",
    "            'max_depth': np.linspace(1,9,3, endpoint=True, dtype = int),\n",
    "            #'min_child_weight':np.linspace(1, 9, 3, endpoint=True),\n",
    "            'colsample_bytree': np.linspace(0.1,0.9,3,endpoint =True)\n",
    "            }\n",
    "    \n",
    "    paramslr = {\n",
    "                }\n",
    "    \n",
    "    paramslas = {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1],\n",
    "            'max_iter': np.linspace(0, 3000, 3, endpoint = True),\n",
    "            'tol': [0.0001, 0.001, 0.01]\n",
    "                }\n",
    "    \n",
    "    paramsrid = {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1],\n",
    "            'max_iter': np.linspace(0, 3000, 3, endpoint = True),\n",
    "            'tol': [0.0001, 0.001, 0.01]\n",
    "                }\n",
    "    \n",
    "    paramsela = {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1],\n",
    "            'max_iter': np.linspace(0, 2000, 10, endpoint = True),\n",
    "            'tol': [0.0001, 0.001, 0.01],\n",
    "            'l1_ratio': np.linspace(0.25,0.75, 5, endpoint = True)\n",
    "            }\n",
    "    \n",
    "    if len(seg_list) == 1 & seg_list[0] == 1:\n",
    "        paramssgd = {\n",
    "            'penalty':['none', 'l1','l2','elasticnet'],\n",
    "            'max_iter': [10, 400, 700, 1000, 2000, 3000],#arttırmayı denedim sonuç değişmedi\n",
    "            #'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "            'n_iter_no_change':[5,10],\n",
    "            'l1_ratio': np.linspace(0.25,0.75,3,endpoint = True)\n",
    "            }\n",
    "    else:\n",
    "        paramssgd = {\n",
    "            'penalty':['none', 'l1','l2','elasticnet'],\n",
    "            'max_iter': np.linspace(0.1, 3000, 3 ,endpoint = True), #arttırmayı denedim sonuç değişmedi\n",
    "            'alpha': [0.001, 0.01, 0.1, 1],\n",
    "            'n_iter_no_change':np.linspace(5, 10, 2, endpoint=True),\n",
    "            'l1_ratio': np.linspace(0.25,0.75,3,endpoint = True)\n",
    "            }\n",
    "    \n",
    "    paramsrf = {\n",
    "            'max_depth':[#5, 9, \n",
    "                         18, 32],\n",
    "            'n_estimators': [#10, 50, \n",
    "                             100, 200],\n",
    "            'min_samples_split': [0.1, 1.0, 2],\n",
    "            'min_samples_leaf': [0.1, 0.5, 1]\n",
    "            }\n",
    "    \n",
    "    paramslgb = {\n",
    "            'max_depth':[5, 9, 18, 32],\n",
    "            'learning_rate': [0.001, 0.01, 0.1],\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "            'num_leaves': np.linspace(11,51,3,endpoint = True, dtype = int)\n",
    "            }\n",
    "    \n",
    "    #models['gbm'] = [XGBRegressor(objective = 'reg:squarederror', booster = 'gbtree', seed = 23, learning_rate = 0.01), paramsgbm]\n",
    "    #models['lr'] = [LinearRegression(), paramslr]\n",
    "    #models['las'] = [Lasso(), paramslas]\n",
    "    #models['rid'] = [Ridge(), paramsrid]\n",
    "    #models['ela'] = [ElasticNet(), paramsela]\n",
    "    #models['huber'] = HuberRegressor()\n",
    "    #models['lars'] = Lars()\n",
    "    #models['passive'] = PassiveAggressiveRegressor()\n",
    "    #models['ransac'] = RANSACRegressor()\n",
    "    models['sgd'] = [SGDRegressor(random_state = 234), paramssgd]\n",
    "    #models['rf'] = [RandomForestRegressor(), paramsrf]\n",
    "    models['lgb'] = [LGBMRegressor(), paramslgb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code tries all active models defined above with their related parameter spaces.\n",
    "\n",
    "First part of the if structure is for a single run scheme. Second part is for a nestedCV scheme where total run size is outerCVinnerCVparam_space . At the last part of if structure, there is a classical gridsearch scheme with a CV. Total run size of this part is CV*param_space .\n",
    "\n",
    "All results are written inside a txt file afterwards for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 882 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_time = 2020-03-17 17:06:20.146478...\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_time = 2020-03-17 17:12:46.050508...\n"
     ]
    }
   ],
   "source": [
    "    for key, value in models.items():\n",
    "        \n",
    "        if hypertest != 1:\n",
    "            \n",
    "            start_time = timeit.default_timer()\n",
    "            \n",
    "            numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (-1,1)))\n",
    "            categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "            #categoric_pipe = make_pipeline(ce.HashingEncoder())\n",
    "            preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "            \n",
    "            all_pipe = make_pipeline(preprocessor, value[0])\n",
    "            \n",
    "            cv = KFold(n_splits = 5, random_state = 23, shuffle = True)\n",
    "            \n",
    "            scorer = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "            \n",
    "            scores = cross_validate(all_pipe, df_x, df_y.values.ravel(), scoring = scorer, cv = cv, return_train_score = True)\n",
    "            \n",
    "            print(\"model = {}\".format(key), file = text_file)\n",
    "            print(\"train_mse = {}, test_mse ={}\".format(np.mean(scores['train_neg_mean_squared_error']), np.mean(scores['test_neg_mean_squared_error'])), file = text_file)\n",
    "            print(\"train_mae = {}, test_mae ={}\".format(np.mean(scores['train_neg_mean_absolute_error']), np.mean(scores['test_neg_mean_absolute_error'])), file = text_file)\n",
    "            print(\"train_r2 = {}, test_r2 = {}\".format(np.mean(scores['train_r2']), np.mean(scores['test_r2'])), file = text_file)\n",
    "            print(\"avg_fit_time = {}\".format(np.mean(scores['fit_time'])), file = text_file)\n",
    "            print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60), file = text_file)\n",
    "            print(\"...{}...\".format(start_time), file = text_file)\n",
    "            print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "            print(\"current_time = {}...\".format(datetime.now()))\n",
    "            print(\"-----------------------------------\", file = text_file)\n",
    "            \n",
    "        else:\n",
    "            if nestedcv == 1:\n",
    "                \n",
    "                start_time = timeit.default_timer()\n",
    "                \n",
    "                numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (-1,1)))\n",
    "                categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "                preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "                \n",
    "                all_pipe = make_pipeline(preprocessor, value[0])\n",
    "                \n",
    "                grid_search = GridSearchCV(all_pipe, value[1], cv=3, verbose=0)\n",
    "                               \n",
    "                cv = KFold(n_splits = 5, random_state = 23, shuffle = False)\n",
    "                 \n",
    "                scorer = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "                #scores = cross_val_score(all_pipe, df_x, df_y.values.ravel(), scoring = 'neg_mean_squared_error', cv=cv)\n",
    "                #r2 = cross_val_score(all_pipe, df_x, df_y.values.ravel(), scoring = 'r2', cv=cv)\n",
    "                scores = cross_validate(grid_search, df_x, df_y.values.ravel(), scoring = scorer, cv = cv, return_train_score = True)\n",
    "                \n",
    "                #you have to fit grid_search object in order to get best_params_ from it. But here you also need a pipeline for preprocessing. \n",
    "                #fitting on pipeline object will give us the desired score. If it was not a pipeline, we would fit grid_search object after preprocessing\n",
    "                scores.fit(df_x, df_y.values.ravel())     \n",
    "                \n",
    "                print(\"model = {}\".format(key), file = text_file)\n",
    "                print(\"train_mse = {}, test_mse ={}\".format(np.mean(scores['train_neg_mean_squared_error']), np.mean(scores['test_neg_mean_squared_error'])), file = text_file)\n",
    "                print(\"train_mse = {}, test_mse ={}\".format(np.mean(scores['train_neg_mean_absolute_error']), np.mean(scores['test_neg_mean_absolute_error'])), file = text_file)\n",
    "                print(\"train_r2 = {}, test_r2 = {}\".format(np.mean(scores['train_r2']), np.mean(scores['test_r2'])), file = text_file)\n",
    "                print(\"avg_fit_time = {}\".format(np.mean(scores['fit_time'])), file = text_file)\n",
    "                #we ar getting best_params_ from grid_search object still, it is valid\n",
    "                print(\"best_params = {}\".format(grid_search.best_params_), file = text_file)\n",
    "                print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60), file = text_file)\n",
    "                print(\"...{}...\".format(start_time), file = text_file)\n",
    "                print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "                print(\"current_time = {}...\".format(datetime.now()))\n",
    "                print(\"-----------------------------------\", file = text_file)\n",
    "                \n",
    "                best_params = dict()\n",
    "                best_params[key] = grid_search.best_params_\n",
    "                \n",
    "                best_results = dict()\n",
    "                best_results[key] = [np.mean(scores['test_neg_mean_squared_error']), np.mean(scores['test_neg_mean_absolute_error']), np.mean(scores['test_r2'])]\n",
    "            \n",
    "            #this part is for only gridsearchcv. We can use CV results, as well as best parameters of the grid search from here. \n",
    "            else:\n",
    "                \n",
    "                start_time = timeit.default_timer()\n",
    "        \n",
    "                scorer = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "        \n",
    "                numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (0,1)))\n",
    "                categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "                preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "\n",
    "                all_pipe = make_pipeline(preprocessor, value[0])\n",
    "                \n",
    "                #njobs = -1 uses 2 cores (all available) and 4 threads; consumes 100% of CPU. But it is faster absolutely. Njobs = 1 uses 1 core only, slower but consumes less CPU\n",
    "                grid_search = GridSearchCV(all_pipe, value[1], cv=5, verbose=1, refit = 'neg_mean_squared_error', scoring = scorer, return_train_score = True, n_jobs = -1)\n",
    "                #scoring option is for defining multiple scorers, otherwise null is OK\n",
    "                #refit must be chosen if multi scorers is selected. But best_score_, best_params_ etc will give only the result of that metric, cant get multi scores\n",
    "                #here REFIT option tells you which metric do you want to consider for best_params_ calculation(according to which metric)\n",
    "\n",
    "                grid_search.fit(df_x_regr, df_y_regr.values.ravel())\n",
    "                #all_pipe.fit(df_x, df_y.values.ravel())\n",
    "\n",
    "                #from the resulting parameters dictionary, we choose the index of best_param_ set. This index will help us the get best param value from subresults\n",
    "                #metric results are in arraf form, in each array there is a list of results corresponding to the all parameter combinations. \n",
    "                ind = grid_search.best_index_      \n",
    "\n",
    "                print(\"model = {}\".format(key), file = text_file)\n",
    "                print(\"train_mse = {}, test_mse ={}\".format(grid_search.cv_results_['mean_train_neg_mean_squared_error'][ind], grid_search.cv_results_['mean_test_neg_mean_squared_error'][ind]), file = text_file)\n",
    "                print(\"train_mae = {}, test_mae ={}\".format(grid_search.cv_results_['mean_train_neg_mean_absolute_error'][ind], grid_search.cv_results_['mean_test_neg_mean_absolute_error'][ind]), file = text_file)\n",
    "                print(\"train_r2 = {}, test_r2 = {}\".format(grid_search.cv_results_['mean_train_r2'][ind], grid_search.cv_results_['mean_test_r2'][ind]), file = text_file)\n",
    "                print(\"avg_fit_time = {}\".format(grid_search.cv_results_['mean_fit_time'][ind]), file = text_file)\n",
    "                #we are getting best_params_ from grid_search object still, it is valid\n",
    "                print(\"best_params = {}\".format(grid_search.best_params_), file = text_file)\n",
    "                print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60), file = text_file)\n",
    "                print(\"...{}...\".format(start_time), file = text_file)\n",
    "                print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "                print(\"current_time = {}...\".format(datetime.now()))\n",
    "                print(\"-----------------------------------\", file = text_file)\n",
    "\n",
    "                best_params = dict()\n",
    "                best_params[key] = grid_search.best_params_\n",
    "\n",
    "                best_results = dict()\n",
    "                best_results[key] = [grid_search.cv_results_['mean_test_neg_mean_squared_error'][ind], grid_search.cv_results_['mean_test_neg_mean_absolute_error'][ind], grid_search.cv_results_['mean_test_r2'][ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if neural_model == 1:\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.wrappers.scikit_learn import KerasRegressor\n",
    "    from keras.optimizers import RMSprop, SGD\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from keras.models import load_model\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    #these are inactive if cross validation will be used. Test set is applied several times for CV.\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(df_x, df_y.values.ravel(), test_size = 0.1, random_state = 23)\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(df_x, df_y.values.ravel(), test_size = 0.25)\n",
    "    \n",
    "    numeric_pipe = make_pipeline(Normalizer())\n",
    "    #numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (-1,1)))\n",
    "    #SPARSE MATRICES MAY BE MORE EFFICIENT BUT IT IS NOT VERY SUITABLE FOR KERAS MODELS\n",
    "    categoric_pipe = make_pipeline(OneHotEncoder(sparse = False, handle_unknown='ignore'))\n",
    "    preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "            \n",
    "    all_pipe2 = make_pipeline(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code is related to the neural network training. Preprocess part starts here. Then there is an inactive single run part below(Cross validation for neural networks is mostly ineffective because of huge datasets but in this case, my dataset is not huge and I could run a CV model easily) and also an active CV run part.NORMALIZATON, and ALSO SCALING AFTERWARDS, IS CRUCIAL FOR NNet preprocessing.!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\nx_train = all_pipe2.fit_transform(x_train)\\nx_test = all_pipe2.transform(x_test)\\n\\n#target_pipe = make_pipeline(Normalizer())\\n\\n#y_train = target_pipe.fit_transform(y_train.reshape(-1,1))\\n#y_test = target_pipe.transform(y_test.reshape(-1,1))\\n#reshape is required for normalization like transformations of 1D arrays(output values for example)\\n\\noptimizer = RMSprop(0.001)\\nsgd = SGD(lr=0.1, momentum=0.8,nesterov=False)\\n  \\nmodel = Sequential()\\nmodel.add(Dense(30, input_dim=x_train.shape[1], kernel_initializer=\\'normal\\', activation=\\'relu\\'))\\nmodel.add(Dense(10, activation=\\'relu\\'))\\nmodel.add(Dense(1, activation=\\'linear\\'))\\nmodel.summary()\\n\\nmodel.compile(loss=\\'mse\\', optimizer=\\'adam\\', metrics=[\\'mse\\',\\'mae\\'])\\n\\npatience = 100\\n#early stopping stops training after n unsuccesful runs. Model checkpoint saves that best model n epochs before termination.\\ncallbacks = [EarlyStopping(monitor=\\'val_mean_squared_error\\', patience=patience),\\n             ModelCheckpoint(filepath=\\'best_model.h5\\', monitor=\\'val_mean_squared_error\\', save_best_only=True)]\\n#callbacks = [EarlyStopping(monitor=\\'val_mean_squared_error\\', patience=patience)]   \\n\\nhistory = model.fit(x_train, y_train, epochs=10000, batch_size=30,  verbose=1, validation_split=0.2, callbacks = callbacks)\\n\\n#scores = model.evaluate(x_test, y_test, verbose=0)\\n#print(scores)\\n#print(model.metrics_names)\\n\\n   \\nplt.figure()\\nplt.xlabel(\\'Epoch\\')\\nplt.ylabel(\\'Mean_Square_Error\\')\\nplt.gca().set_xlim([100, np.max(history.epoch)])\\nplt.plot(history.epoch, history.history[\\'mean_squared_error\\'],label=\\'Train Error\\')\\nplt.plot(history.epoch, history.history[\\'val_mean_squared_error\\'],label = \\'Val Error\\')\\nplt.legend([\\'train\\', \\'validation\\'], loc=\\'upper left\\')\\nplt.show()\\n\\nprint(\"Average of last +-25 best epochs of best epoch:\")\\nprint(np.mean(history.history[\\'val_mean_squared_error\\'][(np.max(history.epoch)-patience-25):(np.max(history.epoch)-patience+25)]))\\nprint(\"............................\")\\n\\nsaved_model = load_model(\\'best_model.h5\\')\\n\\nloss, mse, mae = saved_model.evaluate(x_test, y_test, verbose=0)\\nprint(\"Test set MSE: {}\".format(mse))\\nprint(\"Test set MAE: {}\".format(mae))\\nprint(\"Test set LOSS: {}\".format(loss))\\nprint(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60))\\n\\n### IF you use a CROSS VALIDATION and your results vary so much, this means you need to increase epoch, increase learning rate\\n# or decrease batch size, or all. \\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    '''   \n",
    "    x_train = all_pipe2.fit_transform(x_train)\n",
    "    x_test = all_pipe2.transform(x_test)\n",
    "    \n",
    "    #target_pipe = make_pipeline(Normalizer())\n",
    "    \n",
    "    #y_train = target_pipe.fit_transform(y_train.reshape(-1,1))\n",
    "    #y_test = target_pipe.transform(y_test.reshape(-1,1))\n",
    "    #reshape is required for normalization like transformations of 1D arrays(output values for example)\n",
    "    \n",
    "    optimizer = RMSprop(0.001)\n",
    "    sgd = SGD(lr=0.1, momentum=0.8,nesterov=False)\n",
    "      \n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    \n",
    "    patience = 100\n",
    "    #early stopping stops training after n unsuccesful runs. Model checkpoint saves that best model n epochs before termination.\n",
    "    callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=patience),\n",
    "                 ModelCheckpoint(filepath='best_model.h5', monitor='val_mean_squared_error', save_best_only=True)]\n",
    "    #callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=patience)]   \n",
    "    \n",
    "    history = model.fit(x_train, y_train, epochs=10000, batch_size=30,  verbose=1, validation_split=0.2, callbacks = callbacks)\n",
    "    \n",
    "    #scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    #print(scores)\n",
    "    #print(model.metrics_names)\n",
    "\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean_Square_Error')\n",
    "    plt.gca().set_xlim([100, np.max(history.epoch)])\n",
    "    plt.plot(history.epoch, history.history['mean_squared_error'],label='Train Error')\n",
    "    plt.plot(history.epoch, history.history['val_mean_squared_error'],label = 'Val Error')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Average of last +-25 best epochs of best epoch:\")\n",
    "    print(np.mean(history.history['val_mean_squared_error'][(np.max(history.epoch)-patience-25):(np.max(history.epoch)-patience+25)]))\n",
    "    print(\"............................\")\n",
    "    \n",
    "    saved_model = load_model('best_model.h5')\n",
    "    \n",
    "    loss, mse, mae = saved_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"Test set MSE: {}\".format(mse))\n",
    "    print(\"Test set MAE: {}\".format(mae))\n",
    "    print(\"Test set LOSS: {}\".format(loss))\n",
    "    print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60))\n",
    "    \n",
    "    ### IF you use a CROSS VALIDATION and your results vary so much, this means you need to increase epoch, increase learning rate\n",
    "    # or decrease batch size, or all. \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             code  service from  to         M-9         M-8         M-7  \\\n",
      "54795  0000208908  İhracat   TR  ES    1.410042    0.555957    2.559028   \n",
      "29182       Diğer  İthalat   IT  TR  240.803519  247.654166  296.180263   \n",
      "43384  0000012261  İhracat   TR  BE    0.061031    0.115291    0.031690   \n",
      "57537  0000029672  İthalat   GB  TR    0.000000    3.000000    0.000000   \n",
      "10887  0000026771  İthalat   BE  TR   41.294617   25.203369   28.150530   \n",
      "...           ...      ...  ...  ..         ...         ...         ...   \n",
      "38047  0000046105  İthalat   DE  TR   13.269270    3.811358    0.988860   \n",
      "23207       Diğer  İhracat   TR  EE    1.989568    0.208395    1.066667   \n",
      "62058  0000262119  İhracat   TR  DE   16.527016   14.837986   18.716225   \n",
      "34299  0000190596  İthalat   FR  TR    2.531275    2.203352    2.540599   \n",
      "12347  0000150154  İhracat   TR  CZ    0.000000    0.000000    0.000000   \n",
      "\n",
      "              M-6         M-5         M-4         M-3         M-2         M-1  \\\n",
      "54795    2.000000    2.000000    1.000000    1.628006    1.000000    5.000000   \n",
      "29182  244.233662  152.622872  205.208823  229.785136  224.154405  163.889385   \n",
      "43384    0.000000    0.172840    0.082404    0.092445    0.000000    0.041298   \n",
      "57537    0.000000    0.000000    1.000000    0.000000    1.000000    0.079545   \n",
      "10887   25.675501   19.739059   32.698814   31.587955   26.811008   36.974216   \n",
      "...           ...         ...         ...         ...         ...         ...   \n",
      "38047    0.930533    5.511732    0.909880    0.746412    5.899168    0.852549   \n",
      "23207    0.009836    0.164424    4.189956    0.237862    0.003559    0.258550   \n",
      "62058   17.184038   22.850435   14.504711   20.048796   15.296743   13.459346   \n",
      "34299    1.567800    2.156313    2.121029    1.954929    1.760346    4.222070   \n",
      "12347    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "\n",
      "      segment month quarter     moving3     moving6    moving12     Dolar  \\\n",
      "54795       3     6       2    2.542669    2.104668    2.445896  3.518147   \n",
      "29182       1     1       1  205.942975  203.315714  240.433099  3.769616   \n",
      "43384       3     2       1    0.044581    0.064831    0.073525  3.661479   \n",
      "57537       3     3       1    0.359848    0.346591    0.550548  3.671839   \n",
      "10887       2     6       2   31.791060   28.914426   31.501558  4.629933   \n",
      "...       ...   ...     ...         ...         ...         ...       ...   \n",
      "38047       3     9       3    2.499376    2.475046    3.924719  5.710883   \n",
      "23207       3     4       2    0.166657    0.810698    0.739760  4.060917   \n",
      "62058       2    10       4   16.268295   17.224012   16.372485  3.675823   \n",
      "34299       3     6       2    2.645782    2.297081    2.200047  3.518147   \n",
      "12347       3     2       1    0.000000    0.000000    0.000000  3.785925   \n",
      "\n",
      "           Euro  dptholidaycnt  dptmoncnt  dpttuecnt  dptwedcnt  dptthurscnt  \\\n",
      "54795  3.951720            2.0        3.0        4.0        5.0          5.0   \n",
      "29182  4.598094            1.0        5.0        5.0        4.0          4.0   \n",
      "43384  3.897529            0.0        4.0        4.0        4.0          4.0   \n",
      "57537  3.925942            0.0        4.0        5.0        5.0          5.0   \n",
      "10887  5.404017            0.0        4.0        4.0        4.0          5.0   \n",
      "...         ...            ...        ...        ...        ...          ...   \n",
      "38047  6.287220            0.0        4.0        4.0        4.0          4.0   \n",
      "23207  4.983217            1.0        4.0        4.0        4.0          4.0   \n",
      "62058  4.319777            0.0        5.0        4.0        4.0          4.0   \n",
      "34299  3.951720            1.0        4.0        4.0        5.0          5.0   \n",
      "12347  4.673486            0.0        4.0        4.0        4.0          4.0   \n",
      "\n",
      "       dptfridaycnt  arvholidaycnt  arvmoncnt  arvtuecnt  arvwedcnt  \\\n",
      "54795           4.0            1.0        4.0        4.0        5.0   \n",
      "29182           4.0            1.0        5.0        5.0        4.0   \n",
      "43384           4.0            0.0        4.0        4.0        4.0   \n",
      "57537           4.0            0.0        4.0        5.0        5.0   \n",
      "10887           5.0            1.0        4.0        4.0        4.0   \n",
      "...             ...            ...        ...        ...        ...   \n",
      "38047           4.0            0.0        4.0        4.0        4.0   \n",
      "23207           4.0            0.0        4.0        4.0        4.0   \n",
      "62058           4.0            2.0        3.0        4.0        4.0   \n",
      "34299           4.0            2.0        3.0        4.0        5.0   \n",
      "12347           4.0            0.0        4.0        4.0        4.0   \n",
      "\n",
      "       arvthurscnt  arvfridaycnt      D-1     D-2     D-3  pdptholiday  \\\n",
      "54795          5.0           4.0   400.00  -38.58   62.80          2.0   \n",
      "29182          4.0           4.0   -26.89   -2.45   11.98          4.0   \n",
      "43384          4.0           4.0   100.00 -100.00   12.19          0.0   \n",
      "57537          5.0           4.0   -92.05  100.00 -100.00          0.0   \n",
      "10887          4.0           5.0    37.91  -15.12   -3.40          3.0   \n",
      "...            ...           ...      ...     ...     ...          ...   \n",
      "38047          4.0           4.0   -85.55  690.34  -17.97          2.0   \n",
      "23207          4.0           4.0  7165.25  -98.50  -94.32          0.0   \n",
      "62058          4.0           4.0   -12.01  -23.70   38.22          4.0   \n",
      "34299          5.0           4.0   139.84   -9.95   -7.83          3.0   \n",
      "12347          4.0           4.0     0.00    0.00    0.00          1.0   \n",
      "\n",
      "       parvholiday  pyearsales                   comb  \n",
      "54795          1.0    2.616858  0000208908İhracatTRES  \n",
      "29182          0.0  272.002593       DiğerİthalatITTR  \n",
      "43384          0.0    0.162362  0000012261İhracatTRBE  \n",
      "57537          0.0    1.507082  0000029672İthalatGBTR  \n",
      "10887          1.0   28.620226  0000026771İthalatBETR  \n",
      "...            ...         ...                    ...  \n",
      "38047          4.0    4.007283  0000046105İthalatDETR  \n",
      "23207          1.0    0.294126       DiğerİhracatTREE  \n",
      "62058          0.0   11.293549  0000262119İhracatTRDE  \n",
      "34299          2.0    0.000000  0000190596İthalatFRTR  \n",
      "12347          1.0    0.000000  0000150154İhracatTRCZ  \n",
      "\n",
      "[19346 rows x 40 columns]\n",
      "[0.69314718 5.27079184 0.02658945 ... 3.15699604 1.50796712 0.03883983]\n",
      "             code  service from  to         M-9         M-8         M-7  \\\n",
      "37674  0000060524  İhracat   TR  ES  235.624130  142.647430  228.038406   \n",
      "56525  0000264563  İthalat   DE  TR   12.606728    8.422963    6.825225   \n",
      "41695  0000220868  İthalat   BE  TR    1.000000    0.000000    0.680000   \n",
      "43872  0000012261  İthalat   BE  TR    0.280528    0.078652    0.130991   \n",
      "2886   0000195581  İhracat   TR  BA    0.000000    0.000000    0.000000   \n",
      "...           ...      ...  ...  ..         ...         ...         ...   \n",
      "23148       Diğer  İhracat   TR  DK    8.842777    5.977492    8.441277   \n",
      "34967  0000130051  İhracat   TR  DE   92.000000   53.000000   44.000000   \n",
      "9679   0000069281  İthalat   NL  TR    0.670618    0.786702    0.545217   \n",
      "26315       Diğer  İthalat   BE  TR   38.465905   32.174140   29.408849   \n",
      "60551  0000243968  İthalat   BE  TR   63.078575   26.561937   29.809456   \n",
      "\n",
      "              M-6         M-5         M-4         M-3         M-2         M-1  \\\n",
      "37674  185.343399  292.912415  370.798462  200.918137  159.417877  264.514374   \n",
      "56525    2.457583    2.955823    2.037812    5.015133    9.331200    4.578593   \n",
      "41695    1.000000    0.000000    1.000000    1.253968    0.000000    1.000000   \n",
      "43872    0.023179    0.221765    0.131086    0.140902    0.067308    0.238232   \n",
      "2886     0.000000    0.000000    0.000000    0.000000    1.000000    0.000000   \n",
      "...           ...         ...         ...         ...         ...         ...   \n",
      "23148   19.249471    5.992268    3.261538   10.859696    4.064596   19.393898   \n",
      "34967   57.000000   63.000000   44.000000   61.000000   50.000000   76.000000   \n",
      "9679     0.635507    0.907361    0.371799    0.375871    0.469136    0.227680   \n",
      "26315   27.898709   25.485487   19.465164   19.176076   24.697622   13.825268   \n",
      "60551   34.451744   43.320953   35.677681   35.360348   25.318466   44.444649   \n",
      "\n",
      "      segment month quarter     moving3     moving6    moving12     Dolar  \\\n",
      "37674       1     2       1  208.283463  245.650777  216.831011  5.272211   \n",
      "56525       3     4       2    6.308308    4.396024    6.599541  5.767883   \n",
      "41695       3     9       3    0.751323    0.708995    0.744497  6.306813   \n",
      "43872       3     2       1    0.148814    0.137079    0.147055  3.661479   \n",
      "2886        3     8       3    0.333333    0.166667    0.166667  3.505071   \n",
      "...       ...   ...     ...         ...         ...         ...       ...   \n",
      "23148       2     6       2   11.439397   10.470245    9.444880  4.629933   \n",
      "34967       1     3       1   62.333333   58.500000   63.333333  3.671839   \n",
      "9679        3     6       2    0.357562    0.497892    0.495369  5.816637   \n",
      "26315       2     1       1   19.232989   21.758055   27.962975  3.769616   \n",
      "60551       2     4       2   35.041154   36.428974   40.370961  5.767883   \n",
      "\n",
      "           Euro  dptholidaycnt  dptmoncnt  dpttuecnt  dptwedcnt  dptthurscnt  \\\n",
      "37674  5.984111            0.0        4.0        4.0        4.0          4.0   \n",
      "56525  6.478277            2.0        5.0        4.0        4.0          3.0   \n",
      "41695  7.348957            0.0        4.0        4.0        4.0          4.0   \n",
      "43872  3.897529            0.0        4.0        4.0        4.0          4.0   \n",
      "2886   4.142284            2.0        5.0        4.0        4.0          4.0   \n",
      "...         ...            ...        ...        ...        ...          ...   \n",
      "23148  5.404017            1.0        4.0        4.0        4.0          4.0   \n",
      "34967  3.925942            0.0        4.0        5.0        5.0          5.0   \n",
      "9679   6.569733            1.0        4.0        4.0        4.0          4.0   \n",
      "26315  4.598094            1.0        5.0        5.0        4.0          4.0   \n",
      "60551  6.478277            2.0        5.0        4.0        4.0          4.0   \n",
      "\n",
      "       dptfridaycnt  arvholidaycnt  arvmoncnt  arvtuecnt  arvwedcnt  \\\n",
      "37674           4.0            0.0        4.0        4.0        4.0   \n",
      "56525           4.0            1.0        4.0        4.0        4.0   \n",
      "41695           5.0            0.0        4.0        4.0        4.0   \n",
      "43872           4.0            0.0        4.0        4.0        4.0   \n",
      "2886            4.0            0.0        5.0        5.0        5.0   \n",
      "...             ...            ...        ...        ...        ...   \n",
      "23148           5.0            1.0        3.0        4.0        4.0   \n",
      "34967           4.0            0.0        4.0        5.0        5.0   \n",
      "9679            5.0            5.0        3.0        3.0        3.0   \n",
      "26315           4.0            1.0        5.0        5.0        4.0   \n",
      "60551           4.0            1.0        4.0        4.0        4.0   \n",
      "\n",
      "       arvthurscnt  arvfridaycnt     D-1     D-2     D-3  pdptholiday  \\\n",
      "37674          4.0           4.0   65.93  -20.66  -45.81          1.0   \n",
      "56525          4.0           4.0  -50.93   86.06  146.10          0.0   \n",
      "41695          4.0           5.0  100.00 -100.00   25.40          1.0   \n",
      "43872          4.0           4.0  253.94  -52.23    7.49          0.0   \n",
      "2886           4.0           4.0 -100.00  100.00    0.00          0.0   \n",
      "...            ...           ...     ...     ...     ...          ...   \n",
      "23148          5.0           5.0  377.14  -62.57  232.96          1.0   \n",
      "34967          5.0           4.0   52.00  -18.03   38.64          0.0   \n",
      "9679           3.0           5.0  -51.47   24.81    1.10          1.0   \n",
      "26315          4.0           4.0  -44.02   28.79   -1.49          1.0   \n",
      "60551          4.0           4.0   75.54  -28.40   -0.89          0.0   \n",
      "\n",
      "       parvholiday  pyearsales                   comb  \n",
      "37674          2.0  139.111984  0000060524İhracatTRES  \n",
      "56525          0.0    7.373015  0000264563İthalatDETR  \n",
      "41695          5.0    1.000000  0000220868İthalatBETR  \n",
      "43872          0.0    0.270716  0000012261İthalatBETR  \n",
      "2886           0.0    1.000000  0000195581İhracatTRBA  \n",
      "...            ...         ...                    ...  \n",
      "23148          2.0   10.800687       DiğerİhracatTRDK  \n",
      "34967          0.0   64.000000  0000130051İhracatTRDE  \n",
      "9679           1.0    0.417900  0000069281İthalatNLTR  \n",
      "26315          0.0   29.934638       DiğerİthalatBETR  \n",
      "60551          0.0   40.753544  0000243968İthalatBETR  \n",
      "\n",
      "[4837 rows x 40 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RMSprop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-5e87731b2bd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_x_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0moptimizerr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mcv_x_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_pipe2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RMSprop' is not defined"
     ]
    }
   ],
   "source": [
    "    kfold = KFold(n_splits = 5, random_state = 23, shuffle = True)\n",
    "    \n",
    "    epochbestlist = []\n",
    "    epochbestvallist = []\n",
    "    epochavglist = []\n",
    "    epochvalavglist = []\n",
    "    testmselist = []\n",
    "    testmaelist = []\n",
    "    testlosslist= []\n",
    "    \n",
    "    ### IF you use a CROSS VALIDATION and your results vary so much, this means you need to increase epoch, increase learning rate\n",
    "    # or decrease batch size, or all. \n",
    "    for train_id, test_id in kfold.split(df_x_regr, df_y_regr.values.ravel()):\n",
    "    \n",
    "        patience = 50\n",
    "\n",
    "        cv_x = df_x_regr.iloc[train_id]\n",
    "        cv_x_test = df_x_regr.iloc[test_id]\n",
    "        #normally, if we pass column transformer pipeline into CROSS_VAL_SCORE directly etc, we also define DATAFRAMES with it. but here if we use df_x.values[train_id]\n",
    "        #it is not a dataframe anymore. and we cant pass it into a column transformer pipeline like that. So, I used a datafram filter not to lose the structure. \n",
    "        \n",
    "        cv_y = df_y_regr.values.ravel()[train_id]\n",
    "        cv_y_test = df_y_regr.values.ravel()[test_id]\n",
    "        \n",
    "        print(cv_x)\n",
    "        print(cv_y)\n",
    "        print(cv_x_test)\n",
    "        \n",
    "        optimizerr = RMSprop(0.001)\n",
    "        \n",
    "        cv_x_pre = all_pipe2.fit_transform(cv_x)\n",
    "        cv_x_pre_test = all_pipe2.transform(cv_x_test)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, input_dim=cv_x_pre.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=optimizerr, metrics=['mse','mae'])  \n",
    "        \n",
    "        callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=patience),\n",
    "                 ModelCheckpoint(filepath='best_model.h5', monitor='val_mean_squared_error', save_best_only=True)]  \n",
    "        \n",
    "        history = model.fit(cv_x_pre, cv_y, callbacks = callbacks, epochs = 10000, batch_size = 10, verbose = 1, validation_split = 0.20)\n",
    "        \n",
    "        epochbest = np.min(history.history['mean_squared_error'])\n",
    "        epochvalbest = np.min(history.history['val_mean_squared_error'])\n",
    "        \n",
    "        epochavg = np.mean(history.history['mean_squared_error'][(np.max(history.epoch)-patience-5):(np.max(history.epoch)-patience+5)])\n",
    "        epochvalavg = np.mean(history.history['val_mean_squared_error'][(np.max(history.epoch)-patience-5):(np.max(history.epoch)-patience+5)])\n",
    "        \n",
    "        saved_model = load_model('best_model.h5')\n",
    "        \n",
    "        loss, mse, mae = saved_model.evaluate(cv_x_pre_test, cv_y_test, verbose=0)\n",
    "        \n",
    "        print(\"Best epoch train MSE:\", file = text_file)\n",
    "        print(epochbest, file = text_file)\n",
    "        print(\"............................\", file = text_file)\n",
    "        \n",
    "        print(\"Best epoch validation MSE:\", file = text_file)\n",
    "        print(epochvalbest, file = text_file)\n",
    "        print(\"............................\", file = text_file)\n",
    "        \n",
    "        print(\"Average MSE of last +-25 best epochs of best epoch (train):\", file = text_file)\n",
    "        print(epochavg, file = text_file)\n",
    "        print(\"............................\", file = text_file)\n",
    "        \n",
    "        print(\"Average MSE of last +-25 best epochs of best epoch (validation):\", file = text_file)\n",
    "        print(epochvalavg,file = text_file)\n",
    "        print(\"............................\", file = text_file)\n",
    "\n",
    "        print(\"Test set MSE: {}\".format(mse), file = text_file)\n",
    "        print(\"Test set MAE: {}\".format(mae), file = text_file)\n",
    "        print(\"Test set LOSS: {}\".format(loss), file = text_file)\n",
    "                \n",
    "        epochbestlist.append(epochbest)\n",
    "        epochbestvallist.append(epochvalbest)\n",
    "        epochavglist.append(epochavg)\n",
    "        epochvalavglist.append(epochvalavg)\n",
    "        testmselist.append(mse)\n",
    "        testmaelist.append(mae)\n",
    "        testlosslist.append(loss)\n",
    "        \n",
    "        \n",
    "    print('Mean of epoch best MSE:',np.mean(epochbestlist), file = text_file)\n",
    "    print('Mean of epoch best validation MSE:',np.mean(epochbestvallist), file = text_file)\n",
    "    print('Mean of epoch avg MSE:',np.mean(epochavglist), file = text_file)\n",
    "    print('Mean of epoch avg validation MSE:',np.mean(epochvalavglist), file = text_file)\n",
    "    print('Mean test MSE:',np.mean(testmselist), file = text_file)\n",
    "    print('Mean test MAE:',np.mean(testmaelist), file = text_file)\n",
    "    print('Mean test LOSS:',np.mean(testlosslist), file = text_file)\n",
    "    print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "    print(\"current_time = {}...\".format(datetime.now()))\n",
    "    print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60), file = text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the neural model above with CV, each fold score is stored inside lists. Inside each fold, a patience value is determined and fold automatically stops after P not-improved epochs from best epoch. And the best epoch scores are stored for this fold.\n",
    "\n",
    "After each folds are completed, mean of these best scores are summarized at the end of process.\n",
    "\n",
    "Below code is about finalizing the best model with full dataset and making predictions on it.\n",
    "\n",
    "After determining the best hyperparameters and best model for each of segment with partial datasets at previous section, these hyperparameters are written into this part. Then we are applying a preprocessing part, again, on whole data(seperately for each segment again). Due to the caution of data leakage preprocessing is applied on train data only. In here, train data is whole data. So we have to preprocess them together.\n",
    "\n",
    "As it is a 2 staged model, first one is a classification model and the next one is a regression model, and there are different input datasets for each one, I constructed 2 different pipelines for each preprocessing section.\n",
    "\n",
    "Then code iterates over each company_code, from, to combination. It retrieves data from prediction dataset for a combination C. It gets predictions rowwise. 1st row is applied on classification first, if it is not 0, then it is applied on regression algorithm and gets a prediction. This prediction value (after a reverse transformation is applied, if it is transformed before) is written into available spots for next month values of other rows. (For example it is written into M-1 column for next month's row and it is also written into M-2 column for (i+2)th row etc. With the help of this approach, all empty places inside future months are filled with prediction values.\n",
    "\n",
    "Each combination inside prediction dataset is predicted like this, then they will appear as a single row at the end with predicted values for next N months appearing as columns inside output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if finalize == 1:\n",
    "    \n",
    "    iter_list = []\n",
    "    \n",
    "    if subsets == 1:\n",
    "        iter_list = list(mi.unique_everseen(df_x['segment']))\n",
    "    \n",
    "    else:\n",
    "        iter_list = [0]\n",
    "    \n",
    "    iter_dict = dict()\n",
    "    \n",
    "    iter_dict[0] = [LGBMClassifier(learning_rate = 0.01, max_depth = 32, n_estimators = 200, num_leaves = 51),\n",
    "                    LGBMRegressor(learning_rate= 0.1, max_depth= 18, n_estimators= 200, num_leaves= 11), \n",
    "                    np.log1p(df_y_regr + 10)]\n",
    "    \n",
    "    iter_dict[1] = [LGBMClassifier(learning_rate = 1, max_depth = 5, n_estimators = 200, num_leaves = 11),\n",
    "                    #LGBMRegressor(learning_rate = 0.1, max_depth = 5, n_estimators = 50, num_leaves = 31),    \n",
    "                    #SGDClassifier(alpha = 0.0001, max_iter = 100.0, n_iter_no_change = 5.0, penalty = 'l1', loss = 'log'),\n",
    "                    SGDRegressor(alpha = 0.1, l1_ratio = 0.25, max_iter = 1000, n_iter_no_change = 10.0, penalty = 'l1', random_state = 234), \n",
    "                    df_y_regr]\n",
    "    \n",
    "    iter_dict[2] = [LGBMClassifier(learning_rate = 0.1, max_depth = 32, n_estimators = 100, num_leaves = 51),\n",
    "                    LGBMRegressor(learning_rate = 0.1, max_depth = 5, n_estimators = 100, num_leaves = 11), \n",
    "                    df_y_regr]\n",
    "    \n",
    "    iter_dict[3] = [LGBMClassifier(learning_rate = 0.1, max_depth = 9, n_estimators = 50, num_leaves = 31),\n",
    "                    LGBMRegressor(learning_rate = 0.1, max_depth = 5, n_estimators = 100, num_leaves = 51), \n",
    "                    np.log1p(df_y_regr)]\n",
    "    \n",
    "    \n",
    "    #bu kısımda seçilmiş olan algoritmalara ilişkin detaylar tanımlanıyor\n",
    "    #lgbc = LGBMClassifier(learning_rate = 0.001, max_depth = 5, n_estimators = 50, num_leaves = 11)\n",
    "    #lgbc = LGBMClassifier(learning_rate = 0.1, max_depth = 32, n_estimators = 200, num_leaves = 51)#201912\n",
    "    #lgbc = LGBMClassifier(learning_rate = 0.1, max_depth = 32, n_estimators = 200, num_leaves = 51)#201903\n",
    "    #lgbr = LGBMRegressor(learning_rate = 0.1, max_depth = 5, n_estimators = 200, num_leaves = 31)\n",
    "    #sgdr = SGDRegressor(alpha = 0.01, l1_ratio = 0.75, max_iter = 3000, n_iter_no_change = 5.0, penalty = 'elasticnet')\n",
    "    #lgbr = LGBMRegressor(learning_rate= 0.1, max_depth= 9, n_estimators= 200, num_leaves= 31)#201912\n",
    "    #lgbr = LGBMRegressor(learning_rate= 0.1, max_depth= 18, n_estimators= 200, num_leaves= 11)#201912log1p(y+10)\n",
    "    #lgbr = LGBMRegressor(learning_rate= 0.1, max_depth= 9, n_estimators= 200, num_leaves= 11)#201912log1p(y+10000)\n",
    "    #lgbr = LGBMRegressor(learning_rate= 0.1, max_depth= 9, n_estimators= 200, num_leaves= 31)#201912log1p(y)\n",
    "    \n",
    "    pred_final = pd.DataFrame()\n",
    "    \n",
    "    for q in iter_list:\n",
    "        \n",
    "        if subsets == 1:\n",
    "            df_x_regr_sub = df_x_regr\n",
    "            df_y_regr_sub = iter_dict[q][2]\n",
    "            df_x_sub = df_x\n",
    "            df_y_class_sub = df_y_class\n",
    "            \n",
    "            df_x_regr_sub = df_x_regr.loc[df_x_regr[df_x_regr['segment'] == q].index]\n",
    "            df_y_regr_sub = iter_dict[q][2].loc[df_x_regr[df_x_regr['segment'] == q].index]\n",
    "            \n",
    "            df_x_sub = df_x.loc[df_x[df_x['segment'] == q].index]\n",
    "            df_y_class_sub = df_y_class.loc[df_x[df_x['segment'] == q].index]\n",
    "            \n",
    "            pred_set_sub = pred_set\n",
    "            pred_set_sub = pred_set.loc[pred_set[pred_set['segment']==q].index]\n",
    "            \n",
    "            #df_x_regr_sub.drop(columns = ['segment'], axis = 1, inplace = True)\n",
    "            #df_x_sub.drop(columns = ['segment'], axis = 1, inplace = True)\n",
    "            #df_x_sub.drop(columns = ['segment'], axis = 1, inplace = True)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "                    \n",
    "        numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (0,1)))\n",
    "        categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "        #categoric_pipe = make_pipeline(ce.HashingEncoder())\n",
    "        preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "        \n",
    "        regr_pipe_final = make_pipeline(preprocessor, iter_dict[q][1])\n",
    "        \n",
    "        regr_pipe_final.fit(df_x_regr_sub, df_y_regr_sub.values.ravel())\n",
    "        #regr_pipe_final.fit(df_x, df_y.values.ravel())\n",
    "        \n",
    "        class_pipe_final = make_pipeline(preprocessor, iter_dict[q][0])\n",
    "        print(q)\n",
    "        class_pipe_final.fit(df_x_sub, df_y_class_sub.values.ravel())\n",
    "        \n",
    "        pred_pre_final = pd.DataFrame()\n",
    "        \n",
    "        prob_list = []\n",
    "        #for key in list(mi.unique_everseen(pred_set['from'] + pred_set['from_zip'] + pred_set['to'] + pred_set['to_zip'])):\n",
    "        #burada en başta kenara ayırdığımız tahminleme setindeki her bir satırı tek tek içeriye alıyor;\n",
    "        #yaptığı tahmini de alt satırlarda ilgili yerlere yazıyor. Böylece kayar bir şekilde tahminini yapabiliyor yıl sonuna kadar\n",
    "        for key in list(mi.unique_everseen(pred_set_sub['code'] + pred_set_sub['service'] + pred_set_sub['from'] + pred_set_sub['to'])):\n",
    "            \n",
    "            pred_val_list = []\n",
    "            \n",
    "            pred_subset = pred_set_sub[(pred_set_sub['code'] + pred_set_sub['service'] + pred_set_sub['from'] + pred_set_sub['to']) == key]\n",
    "            #pred_subset = pred_set[(pred_set['code'] + pred_set['comb']) == key]\n",
    "            #pred_subset = pred_subset.reset_index(drop=True)\n",
    "            lag = 0          \n",
    "            \n",
    "            for i in range(0,predmonths):\n",
    "\n",
    "                #iloc[i,:] returns series with columns as indexes for single row output but this returns dataframe with proper indexing\n",
    "                pred = pred_subset.iloc[[i],:]\n",
    "                \n",
    "                for key2, value in movingdict.items():               \n",
    "                    pred.loc[:,'moving'+str(key2)] = (pred[value].sum(axis=1))/key2\n",
    "\n",
    "                pred.drop(columns = pred_drop, axis = 1, inplace = True)\n",
    "\n",
    "                class_val = class_pipe_final.predict(pred)\n",
    "                #class_prob = class_pipe_final.predict_proba(pred)\n",
    "                \n",
    "                regr_val = max(regr_pipe_final.predict(pred), 0)\n",
    "                \n",
    "                transform_dict = { 0 : np.expm1(regr_val) - 10, 1 : regr_val, 2 : regr_val, 3 : np.expm1(regr_val) }\n",
    "                #transform_dict = { 0 : np.expm1(regr_val) - 10, 1 : regr_val, 2 : regr_val, 3 : regr_val, 4 : np.expm1(regr_val)}\n",
    "\n",
    "               # if class_val[0] == 0:\n",
    "                #    prob_list.append(class_prob[0][0])\n",
    "               # else:\n",
    "                #    pass\n",
    "\n",
    "                for j in range (i+1, predmonths+1):\n",
    "    \n",
    "                    if (class_val[0] == 0 & q != 1):\n",
    "                    #if (class_prob[0][0] >= 0.40):\n",
    "                        \n",
    "                        pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = class_val[0]\n",
    "                        \n",
    "                    else:\n",
    "                        if transform == 1 & subsets == 0:\n",
    "                            pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = np.expm1(regr_val[0])\n",
    "                            #pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = np.expm1(regr_val[0]) - 10\n",
    "                            #pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = np.expm1(regr_val[0]) - 10000\n",
    "                            #pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = pow(regr_val[0], 8)\n",
    "                        if subsets == 1:\n",
    "                            #print(pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag])\n",
    "                            #print(iter_dict[q][3])\n",
    "\n",
    "                            pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = transform_dict[q]\n",
    "                \n",
    "                            #print(pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag])\n",
    "                for k in range (1, diff+1):\n",
    "\n",
    "                    pred_subset.iloc[i+1, pred_subset.columns.get_loc(str('D-'+str(k)))] = round((pred_subset.iloc[i+1, pred_subset.columns.get_loc(str('M-'+str(k)))] - \n",
    "                                        pred_subset.iloc[i+1, pred_subset.columns.get_loc(str('M-'+str(k+1)))])*100/\n",
    "                                        pred_subset.iloc[i+1, pred_subset.columns.get_loc(str('M-'+str(k+1)))],2)\n",
    "                    \n",
    "                    pred_subset[str('D-'+str(k))].replace(np.nan, 0, inplace = True)\n",
    "                    pred_subset[str('D-'+str(k))].replace(np.inf, 0, inplace = True)\n",
    "                    \n",
    "                lag = lag + 1\n",
    "            \n",
    "            for key3, value in movingdict.items():               \n",
    "                pred_subset.loc[:,'moving' + str(key3)] = (pred_subset[value].sum(axis=1))/key3\n",
    "            \n",
    "            pred_pre_final = pred_pre_final.append(pred_subset)\n",
    "        \n",
    "        pred_pre_final = pd.merge(pred_pre_final, pred_unique, left_index=True, right_index = True)\n",
    "        pred_pre_final = pred_pre_final[pred_pre_final['date'] == pred_pre_final['date'].max()]\n",
    "        pred_pre_final.drop(columns = final_drop, axis = 1, inplace = True)\n",
    "        pred_final = pred_final.append(pred_pre_final)\n",
    "        \n",
    "    #print_excel = pred_final.to_excel(r'C:\\Users\\ali.kilinc\\Desktop\\12M2020F9M2017Test6AllNew2.xlsx', index = None, header = True, sheet_name = 'FullTable')\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code work for each segment iteratively. Takes all of the data related to segment i, then defined model trained with whole of the data(i). Then prediction are made from pred_set according to trained model. These predicted values are written into next month not-predicted rows in a backwards pattern to fill Past-Month variables like M-1 / M-2 etc. So, that next row can be available to be predicted with the trained model. \n",
    "\n",
    "Final output shows all predicted months with related combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
