{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This forecast model uses the input table created with the \"input transformer code\" applied in another Jupyter notebook. This notebook is about monthly forecasting of sales data. For training and model creation; I used most of the historical data as training data and selected the last N months as test set; N is specified at the beginning of code as a parameter.\n",
    "\n",
    "Test set is consisting of 0 values for test months at beginning. Think about N rows to be predicted. When 1st month of the test set is predicted, it is also written on another rows in a shifting pattern, according to \"Last M months\" parameter set at the beginning. \n",
    "\n",
    "After I get the predictions of model, I compared them to the real life predictions made by sales department for these last N months. \n",
    "\n",
    "Model structure is based on a classification stage at first. Dataset is a \"0 dense dataset\" where about 50% of the data contains 0 as a sales output. This creates some issues for a regression problem. 0 values leverages the predictions towards 0. \n",
    "\n",
    "When I divided the problem into 2 stages; I also could neglect 0 values from regression problem. In first stage, classification algorithm decides if a values is 0 or \"not 0\". If it predicts the value as 0, then a regression algorithm will not be applied for that row and prediction is written a 0. But if it is labeled as \"not 0\", second stage regression algorithm will be applied for that row and regression result will be written as a prediction.\n",
    "\n",
    "Classification dataset consists of all the input data (if it is balanced - daily prediction dataset was imbalanced and I applied (80% 0 20% else) an undersampling algorithm to 0 values to make it balanced, I am adding it into this code also as a comment-) whereas regression data neglects 0 values from the input to eliminate their leverage effect. \n",
    "\n",
    "Inside this development code there are several parameters for experimental trials like nestedcv parameter, or neural network parameter etc. They are set at the beginning according to the experimental design and model is run according to these settings. Mostly, I am using hyperparameter search on several algorithms to get a better performance from this dataset. And also I use cross validation to get a better estimate of errors from different runs. \n",
    "\n",
    "After getting best of the best model among all experiments, a shortened version of this code can be used for further production stages with the best hyperparameters for the selected model, making predictions on newcoming data.\n",
    "\n",
    "This project is simply for ad-hoc usage. It will be used at a monthly frequency to update monthly forecasts for future sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali.kilinc\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns',40)\n",
    "import more_itertools as mi\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning, ConvergenceWarning\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "text_file = open(r'C:\\Users\\ali.kilinc\\Desktop\\Tahminleme\\Output.txt',\"w\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "forecast = pd.read_csv(r'C:\\Users\\ali.kilinc\\Desktop\\Tahminleme\\PrepForecastPWCompany10.csv', index_col= False)\n",
    "\n",
    "param = 12 # last M months parameter to be included as input variable\n",
    "predmonths = 10 # N months to be predicted in a moving pattern\n",
    "transform = 1 #If transformation will be applied to the y values or not\n",
    "pre_classify = 1 #Will pre-classification stage will be applied to the dataset?\n",
    "regress = 1 # will regression stage will be applied\n",
    "hypertest = 1 #Hyperparameter search option is on or off \n",
    "nestedcv = 0 # nestedcv option is on or off\n",
    "neural_model = 0 # will a neural model be applied on dataset?\n",
    "neural_model_final = 0 #If a neural model is trained, after writing the best parameters run a final training on whole dataset\n",
    "finalize = 0 # if a classical ML model is trained, after writing the best parameters, run a final training on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65518 entries, 0 to 65517\n",
      "Data columns (total 41 columns):\n",
      "code             65518 non-null object\n",
      "service          65518 non-null object\n",
      "from             65518 non-null object\n",
      "to               65518 non-null object\n",
      "date             65518 non-null object\n",
      "M-12             65518 non-null int64\n",
      "M-11             65518 non-null int64\n",
      "M-10             65518 non-null float64\n",
      "M-9              65518 non-null float64\n",
      "M-8              65518 non-null float64\n",
      "M-7              65518 non-null float64\n",
      "M-6              65518 non-null float64\n",
      "M-5              65518 non-null float64\n",
      "M-4              65518 non-null float64\n",
      "M-3              65518 non-null float64\n",
      "M-2              65518 non-null float64\n",
      "M-1              65518 non-null float64\n",
      "M                65518 non-null float64\n",
      "month            65518 non-null int64\n",
      "quarter          65518 non-null int64\n",
      "year             65518 non-null int64\n",
      "quarterind       65518 non-null float64\n",
      "monthind         65518 non-null float64\n",
      "moving3          65518 non-null float64\n",
      "moving6          65518 non-null float64\n",
      "moving12         65518 non-null float64\n",
      "Dolar            64124 non-null float64\n",
      "Euro             64124 non-null float64\n",
      "Pound            64124 non-null float64\n",
      "dptholidaycnt    65518 non-null float64\n",
      "dptmoncnt        65518 non-null float64\n",
      "dpttuecnt        65518 non-null float64\n",
      "dptwedcnt        65518 non-null float64\n",
      "dptthurscnt      65518 non-null float64\n",
      "dptfridaycnt     65518 non-null float64\n",
      "arvholidaycnt    65518 non-null float64\n",
      "arvmoncnt        65518 non-null float64\n",
      "arvtuecnt        65518 non-null float64\n",
      "arvwedcnt        65518 non-null float64\n",
      "arvthurscnt      65518 non-null float64\n",
      "arvfridaycnt     65518 non-null float64\n",
      "dtypes: float64(31), int64(5), object(5)\n",
      "memory usage: 20.5+ MB\n",
      "None\n",
      "         code  service   from     to     date          M-12          M-11  \\\n",
      "count   65518    65518  65518  65518    65518  6.551800e+04  6.551800e+04   \n",
      "unique    182        2     38     50       47           NaN           NaN   \n",
      "top     Diğer  İthalat     TR     TR  2016-12           NaN           NaN   \n",
      "freq     7708    35438  29140  30080     1394           NaN           NaN   \n",
      "mean      NaN      NaN    NaN    NaN      NaN  1.491550e+05  1.495050e+05   \n",
      "std       NaN      NaN    NaN    NaN      NaN  7.789153e+05  7.775281e+05   \n",
      "min       NaN      NaN    NaN    NaN      NaN  0.000000e+00  0.000000e+00   \n",
      "25%       NaN      NaN    NaN    NaN      NaN  0.000000e+00  0.000000e+00   \n",
      "50%       NaN      NaN    NaN    NaN      NaN  0.000000e+00  1.000000e+02   \n",
      "75%       NaN      NaN    NaN    NaN      NaN  5.150000e+04  5.310000e+04   \n",
      "max       NaN      NaN    NaN    NaN      NaN  2.634870e+07  2.634870e+07   \n",
      "\n",
      "                M-10           M-9           M-8           M-7           M-6  \\\n",
      "count   6.551800e+04  6.551800e+04  6.551800e+04  6.551800e+04  6.551800e+04   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean    1.471148e+05  1.443046e+05  1.416446e+05  1.390530e+05  1.361936e+05   \n",
      "std     7.709094e+05  7.622078e+05  7.543122e+05  7.468415e+05  7.377876e+05   \n",
      "min     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%     5.040000e+04  4.820000e+04  4.760000e+04  4.760000e+04  4.760000e+04   \n",
      "max     2.634870e+07  2.634870e+07  2.634870e+07  2.634870e+07  2.634870e+07   \n",
      "\n",
      "                 M-5           M-4           M-3           M-2           M-1  \\\n",
      "count   6.551800e+04  6.551800e+04  6.551800e+04  6.551800e+04  6.551800e+04   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean    1.335164e+05  1.309769e+05  1.281070e+05  1.250212e+05  1.219562e+05   \n",
      "std     7.312453e+05  7.231560e+05  7.146723e+05  7.048201e+05  6.941579e+05   \n",
      "min     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%     4.610000e+04  4.200000e+04  3.850000e+04  3.530000e+04  3.190000e+04   \n",
      "max     2.634870e+07  2.634870e+07  2.634870e+07  2.634870e+07  2.634870e+07   \n",
      "\n",
      "                   M         month       quarter  ...    quarterind  \\\n",
      "count   6.551800e+04  65518.000000  65518.000000  ...  65518.000000   \n",
      "unique           NaN           NaN           NaN  ...           NaN   \n",
      "top              NaN           NaN           NaN  ...           NaN   \n",
      "freq             NaN           NaN           NaN  ...           NaN   \n",
      "mean    1.190591e+05      6.382979      2.468085  ...      1.000000   \n",
      "std     6.852879e+05      3.393093      1.108027  ...      0.141441   \n",
      "min     0.000000e+00      1.000000      1.000000  ...      0.101846   \n",
      "25%     0.000000e+00      3.000000      1.000000  ...      1.000000   \n",
      "50%     0.000000e+00      6.000000      2.000000  ...      1.000000   \n",
      "75%     2.880000e+04      9.000000      3.000000  ...      1.000000   \n",
      "max     2.634870e+07     12.000000      4.000000  ...      2.364555   \n",
      "\n",
      "            monthind       moving3       moving6      moving12         Dolar  \\\n",
      "count   65518.000000  6.551800e+04  6.551800e+04  6.551800e+04  64124.000000   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean        1.000000  1.250282e+05  1.292952e+05  1.372124e+05      4.223104   \n",
      "std         0.229158  6.936292e+05  6.986268e+05  7.093417e+05      1.090389   \n",
      "min         0.006743  0.000000e+00  0.000000e+00  0.000000e+00      2.839340   \n",
      "25%         1.000000  0.000000e+00  0.000000e+00  2.250000e+02      3.471887   \n",
      "50%         1.000000  8.333333e+02  3.500000e+03  6.516667e+03      3.779592   \n",
      "75%         1.000000  3.976667e+04  4.760000e+04  5.852292e+04      5.376297   \n",
      "max         5.790202  2.575017e+07  2.464005e+07  2.293493e+07      6.326773   \n",
      "\n",
      "                Euro         Pound  dptholidaycnt     dptmoncnt     dpttuecnt  \\\n",
      "count   64124.000000  64124.000000   65518.000000  65518.000000  65518.000000   \n",
      "unique           NaN           NaN            NaN           NaN           NaN   \n",
      "top              NaN           NaN            NaN           NaN           NaN   \n",
      "freq             NaN           NaN            NaN           NaN           NaN   \n",
      "mean        4.803184      5.524677       0.882643      4.170152      4.217055   \n",
      "std         1.263976      1.323306       1.218630      0.557911      0.547404   \n",
      "min         3.210619      3.808819       0.000000      3.000000      2.000000   \n",
      "25%         3.690652      4.373061       0.000000      4.000000      4.000000   \n",
      "50%         4.562224      5.162018       0.000000      4.000000      4.000000   \n",
      "75%         6.135403      6.786923       1.000000      5.000000      5.000000   \n",
      "max         7.379827      8.273567       6.000000      5.000000      5.000000   \n",
      "\n",
      "           dptwedcnt   dptthurscnt  dptfridaycnt  arvholidaycnt     arvmoncnt  \\\n",
      "count   65518.000000  65518.000000  65518.000000   65518.000000  65518.000000   \n",
      "unique           NaN           NaN           NaN            NaN           NaN   \n",
      "top              NaN           NaN           NaN            NaN           NaN   \n",
      "freq             NaN           NaN           NaN            NaN           NaN   \n",
      "mean        4.197930      4.204722      4.341723       0.865579      4.169953   \n",
      "std         0.557201      0.529183      0.485298       1.226859      0.557301   \n",
      "min         2.000000      2.000000      3.000000       0.000000      3.000000   \n",
      "25%         4.000000      4.000000      4.000000       0.000000      4.000000   \n",
      "50%         4.000000      4.000000      4.000000       0.000000      4.000000   \n",
      "75%         5.000000      5.000000      5.000000       1.000000      5.000000   \n",
      "max         5.000000      5.000000      5.000000       6.000000      5.000000   \n",
      "\n",
      "           arvtuecnt     arvwedcnt   arvthurscnt  arvfridaycnt  \n",
      "count   65518.000000  65518.000000  65518.000000  65518.000000  \n",
      "unique           NaN           NaN           NaN           NaN  \n",
      "top              NaN           NaN           NaN           NaN  \n",
      "freq             NaN           NaN           NaN           NaN  \n",
      "mean        4.217574      4.204371      4.206294      4.341494  \n",
      "std         0.546584      0.553163      0.527141      0.485852  \n",
      "min         2.000000      2.000000      2.000000      3.000000  \n",
      "25%         4.000000      4.000000      4.000000      4.000000  \n",
      "50%         4.000000      4.000000      4.000000      4.000000  \n",
      "75%         5.000000      5.000000      5.000000      5.000000  \n",
      "max         5.000000      5.000000      5.000000      5.000000  \n",
      "\n",
      "[11 rows x 41 columns]\n",
      "Index(['code', 'service', 'from', 'to', 'date', 'M-12', 'M-11', 'M-10', 'M-9',\n",
      "       'M-8', 'M-7', 'M-6', 'M-5', 'M-4', 'M-3', 'M-2', 'M-1', 'M', 'month',\n",
      "       'quarter', 'year', 'quarterind', 'monthind', 'moving3', 'moving6',\n",
      "       'moving12', 'Dolar', 'Euro', 'Pound', 'dptholidaycnt', 'dptmoncnt',\n",
      "       'dpttuecnt', 'dptwedcnt', 'dptthurscnt', 'dptfridaycnt',\n",
      "       'arvholidaycnt', 'arvmoncnt', 'arvtuecnt', 'arvwedcnt', 'arvthurscnt',\n",
      "       'arvfridaycnt'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "forecast['code'] = forecast['code'].astype('str')\n",
    "\n",
    "#when an xls file converted to csv, leading zeros of the text values are lost This line tries to get them back dynamically\n",
    "forecast['code'][forecast['code'].str[:1].str.isnumeric() == True]  = forecast['code'][forecast['code'].str[:1].str.isnumeric() == True].str.zfill(10)\n",
    "\n",
    "print(forecast.info())\n",
    "print(forecast.describe(include='all'))\n",
    "print(forecast.columns)\n",
    "\n",
    "# extracting binary column from a numeric column with a condition\n",
    "forecast['yclass'] = (forecast['M'] > 0).astype(int)\n",
    "#forecast['yclass'] = np.where(forecast['M'] > 0, 1, 0)\n",
    "#forecast['yclass'] = forecast['M'].apply(lambda x: 1 if x > 0 else 0) it is slower than others\n",
    "\n",
    "forecast['date'] = pd.to_datetime(forecast['date'], format = '%Y-%m') #this one transforms to yyyy mm dd format in default. datetime type cannot be converted into mm - yyyyy\n",
    "forecast['date'] = forecast['date'].dt.to_period('m')\n",
    "\n",
    "pred_set = forecast[(forecast['date'] <= forecast['date'].max()) & (forecast['date'] > forecast['date'].max() - (predmonths+1))]\n",
    "forecast = forecast[forecast['date'] <= forecast['date'].max() - (predmonths+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_drop = []\n",
    "\n",
    "for col in forecast.columns:\n",
    "    \n",
    "    if ('M-' in col):\n",
    "        \n",
    "        if (int(re.findall('\\d+', col)[0]) > param) & (int(re.findall('\\d+', col)[0]) > predmonths):\n",
    "            forecast.drop(col, axis = 1, inplace = True)\n",
    "            pred_set.drop(col, axis = 1, inplace = True)\n",
    "            \n",
    "        elif (int(re.findall('\\d+', col)[0]) > param) & (int(re.findall('\\d+', col)[0]) <= predmonths):\n",
    "            forecast.drop(col, axis = 1, inplace = True)\n",
    "            pred_drop.append(col)\n",
    "            \n",
    "        elif (int(re.findall('\\d+', col)[0]) <= param) & (int(re.findall('\\d+', col)[0]) > predmonths):\n",
    "            #pred_set.drop(col, axis = 1, inplace = True)\n",
    "            pred_drop.append(col)\n",
    "                       \n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code dynamically sets Past Month input values on training set and test set. Some of them are totally not needed and dropped directly. Some of them are inserted into a list of columns to be dropped while inserting prediction values to test set at the end.\n",
    "\n",
    "Below code drops some unwanted features from the model. Also plotting part is inactive in this version, they were used for the analysis of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code', 'service', 'from', 'to', 'date', 'M-12', 'M-11', 'M-10', 'M-9',\n",
      "       'M-8', 'M-7', 'M-6', 'M-5', 'M-4', 'M-3', 'M-2', 'M-1', 'M', 'month',\n",
      "       'quarter', 'year', 'quarterind', 'monthind', 'moving3', 'moving6',\n",
      "       'moving12', 'Dolar', 'Euro', 'Pound', 'dptholidaycnt', 'dptmoncnt',\n",
      "       'dpttuecnt', 'dptwedcnt', 'dptthurscnt', 'dptfridaycnt',\n",
      "       'arvholidaycnt', 'arvmoncnt', 'arvtuecnt', 'arvwedcnt', 'arvthurscnt',\n",
      "       'arvfridaycnt', 'yclass'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfig, ax = plt.subplots(2,2)\\n\\nax[0,0] = forecast['dptholidaycnt'].value_counts().plot(kind = 'bar', ax = ax[0,0])\\n\\nax[0,1] = forecast['arvholidaycnt'].value_counts().plot(kind = 'bar', ax = ax[0,1])\\n\\nax[1,0] = forecast['dptfridaycnt'].value_counts().plot(kind = 'bar', ax = ax[1,0])\\n\\nax[1,1] = forecast['arvfridaycnt'].value_counts().plot(kind = 'bar', ax = ax[1,1])\\n\\n\\n\\nfig2, ax2 = plt.subplots(1,2)\\n\\nax2[0].boxplot(forecast['Dolar'])\\n\\nax2[1].boxplot(forecast['Euro'])\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast.drop(columns= ['date', 'year', 'quarterind', 'monthind'], inplace = True)\n",
    "print(pred_set.columns)\n",
    "pred_unique = pred_set['date']\n",
    "pred_set.drop(columns= ['date', 'year', 'quarterind', 'monthind'], inplace = True)\n",
    "\n",
    "''' related to first version, but as a library it can stay\n",
    "for i in range(0,param):\n",
    "    if i < param - 1:\n",
    "        pred_set[str('M-'+str(param - i))] = pred_set[str('M-'+str(param - i - 1))]\n",
    "    else:\n",
    "        pred_set['M-1'] = pred_set['M']\n",
    "'''\n",
    "\n",
    "pred_set.drop(columns= ['M'], inplace = True)\n",
    "\n",
    "'''\n",
    "fig, ax = plt.subplots(2,2)\n",
    "\n",
    "ax[0,0] = forecast['dptholidaycnt'].value_counts().plot(kind = 'bar', ax = ax[0,0])\n",
    "\n",
    "ax[0,1] = forecast['arvholidaycnt'].value_counts().plot(kind = 'bar', ax = ax[0,1])\n",
    "\n",
    "ax[1,0] = forecast['dptfridaycnt'].value_counts().plot(kind = 'bar', ax = ax[1,0])\n",
    "\n",
    "ax[1,1] = forecast['arvfridaycnt'].value_counts().plot(kind = 'bar', ax = ax[1,1])\n",
    "\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(1,2)\n",
    "\n",
    "ax2[0].boxplot(forecast['Dolar'])\n",
    "\n",
    "ax2[1].boxplot(forecast['Euro'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the columns are concatenated as a different column to get combination information directly\n",
    "forecast['comb'] = (forecast['code'] + forecast['from'] + forecast['to'])\n",
    "pred_set['comb'] = (pred_set['code'] + pred_set['from'] + pred_set['to'])\n",
    "\n",
    "#preparing the moving averages dictionary\n",
    "movingdict = dict()\n",
    "\n",
    "for w in [3,6,12]:\n",
    "    movinglist = []\n",
    "    for col in pred_set.columns:\n",
    "        if ('M-' in col):\n",
    "            if (int(re.findall('\\d+', col)[0])) <= w:\n",
    "                movinglist.append(col)\n",
    "    movingdict[w] = movinglist\n",
    "\n",
    "#forecast.drop(columns = ['from' ,'to'], inplace = True)\n",
    "#pred_set.drop(columns = ['from' ,'to'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#this code part uses only comb column, transforms a comb name to other if it is above threshold according to 0 values among timespan\\n# this coluld be done with the code line at table preparation code. This would not delete the line but change the combination at beginning\\n# same can be applied to from and to seperately. \\nforecast.drop(columns = ['from', 'to'], axis=1, inplace = True)\\n\\nthreshold = 0.9\\n\\ncomblist = sorted(set(forecast['comb']))\\ncountlist = []\\nfor item in comblist:\\n    #subdf = forecast[['comb', str('M-' + str(param)), 'M']][forecast['comb'] == item]\\n    subdf1 = forecast[['comb', str('M-' + str(param))]][forecast['comb'] == item]\\n    subdf2 = forecast[['comb', 'M']][forecast['comb'] == item]\\n    subdf2.columns = ['comb', str('M-' + str(param))]\\n    #subdf[['comb', str('M-' + str(param))]] = subdf[['comb', str('M-' + str(param))]].append(subdf[['comb','M']].iloc[-param:], ignore_index = True)\\n    subdf1 = subdf1.append(subdf2.iloc[-param:], ignore_index = True)\\n    templist = [item, len(subdf1[subdf1[str('M-' + str(param))] == 0])/len(subdf1)]\\n    countlist.append(templist)\\n    templist = []\\n\\ncountdf = pd.DataFrame(countlist, columns = ['comb', 'zerocnt'])\\n#print(list(countdf['comb'][countdf['zerocnt'] > threshold]))\\n\\ncountdf['zerocnt'].plot(kind = 'hist')\\n#print(countdf['comb'].value_counts())\\n\\nforecast['comb'].replace(list(countdf['comb'][countdf['zerocnt'] > threshold]) , 'oth', inplace = True)\\n#print(forecast['comb'].value_counts().head(170))\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for some ad-hoc visualizations. I used them for analysis and deactivated then\n",
    "\n",
    "'''\n",
    "#I tried to make a dynamic figure which I can zoom in at the specific areas but I couldn't right now. After saving into computer, it was possible. \n",
    "#So i decided to divide all graphs to multiple subplots with a dynamic formula. It was more interpretable. \n",
    "\n",
    "figcnt = ceil(len(sorted(set(forecast['comb'])))/35)\n",
    "plotcnt = 0\n",
    "\n",
    "for m in range(figcnt):\n",
    "    fig = str('fig'+'_'+str(figcnt))\n",
    "    ax = str('ax'+'_'+str(figcnt))\n",
    "    fig, ax = plt.subplots(7,5)\n",
    "    plt.subplots_adjust(left = 0.03, right = 0.95, top = 0.95, bottom = 0.05)\n",
    "    for i ,j in itertools.product(range(7), range(5)):\n",
    "        if plotcnt < len(sorted(set(forecast['comb']))):\n",
    "            ax[i,j] = forecast[['comb', 'M']][forecast['comb']==sorted(set(forecast['comb']))[plotcnt]].plot(kind = 'hist', ax = ax[i,j])\n",
    "            ax[i,j].set(title = sorted(set(forecast['comb']))[plotcnt])\n",
    "            plotcnt += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "'''\n",
    "'''\n",
    "#this code part uses only comb column, transforms a comb name to other if it is above threshold according to 0 values among timespan\n",
    "# this coluld be done with the code line at table preparation code. This would not delete the line but change the combination at beginning\n",
    "# same can be applied to from and to seperately. \n",
    "forecast.drop(columns = ['from', 'to'], axis=1, inplace = True)\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "comblist = sorted(set(forecast['comb']))\n",
    "countlist = []\n",
    "for item in comblist:\n",
    "    #subdf = forecast[['comb', str('M-' + str(param)), 'M']][forecast['comb'] == item]\n",
    "    subdf1 = forecast[['comb', str('M-' + str(param))]][forecast['comb'] == item]\n",
    "    subdf2 = forecast[['comb', 'M']][forecast['comb'] == item]\n",
    "    subdf2.columns = ['comb', str('M-' + str(param))]\n",
    "    #subdf[['comb', str('M-' + str(param))]] = subdf[['comb', str('M-' + str(param))]].append(subdf[['comb','M']].iloc[-param:], ignore_index = True)\n",
    "    subdf1 = subdf1.append(subdf2.iloc[-param:], ignore_index = True)\n",
    "    templist = [item, len(subdf1[subdf1[str('M-' + str(param))] == 0])/len(subdf1)]\n",
    "    countlist.append(templist)\n",
    "    templist = []\n",
    "\n",
    "countdf = pd.DataFrame(countlist, columns = ['comb', 'zerocnt'])\n",
    "#print(list(countdf['comb'][countdf['zerocnt'] > threshold]))\n",
    "\n",
    "countdf['zerocnt'].plot(kind = 'hist')\n",
    "#print(countdf['comb'].value_counts())\n",
    "\n",
    "forecast['comb'].replace(list(countdf['comb'][countdf['zerocnt'] > threshold]) , 'oth', inplace = True)\n",
    "#print(forecast['comb'].value_counts().head(170))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating input and output variables\n",
    "df_y = forecast['M']\n",
    "df_y_class = forecast['yclass']\n",
    "df_x = forecast.drop(columns = ['M', 'yclass'], axis=1)\n",
    "pred_set = pred_set.drop(columns = ['yclass'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables like quarter, month, weekday are actually cyclic variables besides being categorical variables. Below I made experiments for these variables, either being cyclic or categorical. At the end, model performance was better when they were treated as categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#!!!!!!!! IT IS THE TRANSFORMATION OF CYCLICAL VARIABLES LIKE MONTH AND QUARTER.\n",
    "df_x['month_x'] = np.sin(2*np.pi*df_x['month']/12)\n",
    "df_x['month_y'] = np.cos(2*np.pi*df_x['month']/12)\n",
    "\n",
    "df_x['quarter_x'] = np.sin(2*np.pi*df_x['quarter']/4)\n",
    "df_x['quarter_y'] = np.sin(2-np.pi*df_x['quarter']/4)\n",
    "\n",
    "pred_set['month_x'] = np.sin(2*np.pi*pred_set['month']/12)\n",
    "pred_set['month_y'] = np.cos(2*np.pi*pred_set['month']/12)\n",
    "\n",
    "pred_set['quarter_x'] = np.sin(2*np.pi*pred_set['quarter']/4)\n",
    "pred_set['quarter_y'] = np.sin(2-np.pi*pred_set['quarter']/4)\n",
    "\n",
    "df_x.drop(columns = ['month', 'quarter'], axis = 1, inplace = True)\n",
    "pred_set.drop(columns = ['month', 'quarter'], axis = 1, inplace = True)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# General function for this transformation\n",
    "def encode(data, col, max_val): # max val is theoretical maximum of column, for month it is 12, for day of year it is 365 etc.\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data\n",
    "'''\n",
    "\n",
    "df_x['month'] = df_x['month'].astype('category')\n",
    "df_x['quarter'] = df_x['quarter'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quarter', 'comb', 'from', 'to', 'month', 'code', 'service']\n",
      "['dptwedcnt', 'arvfridaycnt', 'dptthurscnt', 'moving12', 'M-11', 'dptfridaycnt', 'moving3', 'Euro', 'arvmoncnt', 'arvwedcnt', 'dpttuecnt', 'M-3', 'M-9', 'M-10', 'M-1', 'Dolar', 'dptmoncnt', 'arvholidaycnt', 'Pound', 'dptholidaycnt', 'moving6', 'M-5', 'M-7', 'M-12', 'arvthurscnt', 'M-2', 'M-8', 'M-6', 'arvtuecnt', 'M-4']\n"
     ]
    }
   ],
   "source": [
    "num_cols = list(df_x._get_numeric_data().columns)\n",
    "\n",
    "fix_cols = []\n",
    "for col in df_x.columns:\n",
    "    if df_x[col].nunique() == 1:\n",
    "        fix_cols.append(col)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "emp_cols = []\n",
    "for col in df_x.columns:\n",
    "    if df_x[col].count() > 0:\n",
    "        pass\n",
    "    else:\n",
    "        emp_cols.append(col)\n",
    "        \n",
    "cat_cols = list(set(df_x.columns) - set(num_cols) - set(emp_cols) - set(fix_cols))\n",
    "num_cols = list(set(num_cols) - set(fix_cols) - set(emp_cols))\n",
    "\n",
    "df_x.drop(columns = list(emp_cols+fix_cols), inplace = True)\n",
    "\n",
    "print(cat_cols)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#On daily dataset, proportion of 0 values to whole dataset is about 75% which means an imbalance on classification dataset\\n#For algorithm, it is much easier to label most of the data as 0, and this means sacrificing from quality.\\n#Because of that reason, some of the 0 values are removed from dataset with RandomUnderSampling.\\nrus = RandomUnderSampler(random_state = 23, sampling_strategy = 1)\\narr_x, arr_y_class = rus.fit_resample(df_x.to_numpy(), df_y_class.to_numpy())\\n\\ndf_x = pd.DataFrame(arr_x, columns = df_x.columns)\\ndf_y_class = pd.DataFrame(arr_y_class, columns = ['yclass'])\\ndf_x[['quarter', 'month', 'weekday']] = df_x[['quarter', 'month', 'weekday']].astype('category')\\ndf_x[num_cols] = df_x[num_cols].astype('float64')\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = pd.DataFrame(df_x, columns = df_x.columns)\n",
    "df_y = pd.DataFrame(df_y, columns = ['M'])\n",
    "df_y_class = pd.DataFrame(df_y_class, columns = ['yclass'])\n",
    "\n",
    "#data shuffler\n",
    "idx = np.random.permutation(df_x.index)\n",
    "df_x = df_x.reindex(idx)\n",
    "df_y = df_y.reindex(idx)\n",
    "df_y_class = df_y_class.reindex(idx)\n",
    "\n",
    "df_x_regr = df_x.loc[df_y_class[df_y_class['yclass']==1].index]\n",
    "df_y_regr = df_y.loc[df_y_class[df_y_class['yclass']==1].index]\n",
    "\n",
    "'''\n",
    "#On daily dataset, proportion of 0 values to whole dataset is about 75% which means an imbalance on classification dataset\n",
    "#For algorithm, it is much easier to label most of the data as 0, and this means sacrificing from quality.\n",
    "#Because of that reason, some of the 0 values are removed from dataset with RandomUnderSampling.\n",
    "rus = RandomUnderSampler(random_state = 23, sampling_strategy = 1)\n",
    "arr_x, arr_y_class = rus.fit_resample(df_x.to_numpy(), df_y_class.to_numpy())\n",
    "\n",
    "df_x = pd.DataFrame(arr_x, columns = df_x.columns)\n",
    "df_y_class = pd.DataFrame(arr_y_class, columns = ['yclass'])\n",
    "df_x[['quarter', 'month', 'weekday']] = df_x[['quarter', 'month', 'weekday']].astype('category')\n",
    "df_x[num_cols] = df_x[num_cols].astype('float64')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M-12', 'M-11', 'M-10', 'M-9', 'M-8', 'M-7', 'M-6', 'M-5', 'M-4', 'M-3', 'M-2', 'M-1']\n"
     ]
    }
   ],
   "source": [
    "month_var = []\n",
    "\n",
    "for i in range(0,param):\n",
    "    month_var.append(str('M-'+str(param -i)))\n",
    "    \n",
    "print(month_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code', 'service', 'from', 'to', 'M-12', 'M-11', 'M-10', 'M-9', 'M-8',\n",
      "       'M-7', 'M-6', 'M-5', 'M-4', 'M-3', 'M-2', 'M-1', 'month', 'quarter',\n",
      "       'moving3', 'moving6', 'moving12', 'Dolar', 'Euro', 'Pound',\n",
      "       'dptholidaycnt', 'dptmoncnt', 'dpttuecnt', 'dptwedcnt', 'dptthurscnt',\n",
      "       'dptfridaycnt', 'arvholidaycnt', 'arvmoncnt', 'arvtuecnt', 'arvwedcnt',\n",
      "       'arvthurscnt', 'arvfridaycnt', 'comb'],\n",
      "      dtype='object')\n",
      "1    27207\n",
      "0    22977\n",
      "Name: yclass, dtype: int64\n",
      "yclass\n",
      "0    22977\n",
      "1    27207\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#y is a lognormal distribution with mostly 0 values, i will try to fit a log(x+1) transformation for that.( same for all month variables)\n",
    "#log1p(x) is equal to log(x+1). reverse of it is expm1(y)\n",
    "#log transformation, or similars, can be applied to whole data as it has no data dependent variable that can lead to a data leakage!!!\n",
    "#Transform input features if needed, but it is not an assumption of regression(normality of ind variables)\n",
    "#highly skewed variables can be normalized with transformations to increase predictive power!!!!!\n",
    "#https://www.researchgate.net/post/Should_I_transform_non-normal_independent_variables_in_logistic_regression/564f3cb65e9d97d2a58b457d/citation/download\n",
    "\n",
    "if transform == 1:\n",
    "    #stats.probplot(df_y_regr.values.ravel(),plot=plt)\n",
    "    #sns.distplot(df_y_regr, hist = False, kde = True)\n",
    "    #plt.show()\n",
    "    #df_y_regr = np.log1p(df_y_regr)\n",
    "    df_y_regr = np.log(df_y_regr)\n",
    "    #df_y_regr = pow(df_y_regr, 0.5)\n",
    "    #stats.probplot(df_y_regr.values.ravel(),plot=plt)\n",
    "    #sns.distplot(df_y_regr, hist = False, kde = True)\n",
    "    #plt.show()\n",
    "    #df_x[month_var] = np.log(df_x[month_var]+1)\n",
    "    #pred_set[month_var] = np.log(pred_set[month_var]+1)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print(pred_set.columns)\n",
    "print(df_y_class['yclass'].value_counts())\n",
    "print(df_y_class.groupby(['yclass']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted', 'max_error', 'mutual_info_score', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_gamma_deviance', 'neg_mean_poisson_deviance', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'roc_auc_ovo', 'roc_auc_ovo_weighted', 'roc_auc_ovr', 'roc_auc_ovr_weighted', 'v_measure_score']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, cohen_kappa_score\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn.metrics as m\n",
    "from lightgbm import LGBMClassifier\n",
    "import category_encoders as ce\n",
    "\n",
    "print(sorted(m.SCORERS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_classify == 1:\n",
    "    \n",
    "    modelsc = dict()\n",
    "    \n",
    "    paramsrfc = {\n",
    "            'max_depth':[5, 9, 18, 32],\n",
    "            'n_estimators':[10, 50, 100, 200],\n",
    "            'min_samples_split': np.linspace(0.1, 1.0, 3, endpoint=True),\n",
    "            'min_samples_leaf':np.linspace(0.1, 0.5, 3, endpoint=True)\n",
    "            }\n",
    "    \n",
    "    paramslgbc = {\n",
    "            'max_depth':[5, 9, 18, 32],\n",
    "            'learning_rate': [0.001, 0.01, 0.1],\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': np.linspace(11,51,3,endpoint = True, dtype = int)\n",
    "            }\n",
    "    \n",
    "    paramsknnc = {'n_neighbors':[6,12,20]}\n",
    "    \n",
    "    paramssgdc = {\n",
    "            'penalty':['l1','l2','elasticnet'],\n",
    "            'max_iter': np.linspace(100,700,3,endpoint = True),\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'n_iter_no_change':np.linspace(5, 17, 3, endpoint=True)\n",
    "            }\n",
    "    \n",
    "    paramsxgbc = {\n",
    "            'n_estimator': np.linspace(100,500,2,endpoint = True),\n",
    "            'max_depth': np.linspace(3,15,3, endpoint=True, dtype = int),\n",
    "            #'min_child_weight':np.linspace(1, 9, 3, endpoint=True),\n",
    "            #'colsample_bytree': np.linspace(0.3,0.5,3,endpoint =True),\n",
    "            'learning_rate': [0.001, 0.01, 0.1]\n",
    "            }\n",
    "    \n",
    "    paramslogitc = {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l2'],\n",
    "            'tol': [0.0001, 0.001, 0.01]\n",
    "            }\n",
    "    \n",
    "    #random forest-logit has given \"AttributeError, none object has no module named write\" error with njobs = -1 for gridsearch and verbose = 1 for Random Forest. I closed verbose\n",
    "    #modelsc['rf'] = [RandomForestClassifier(), paramsrfc]\n",
    "    modelsc['lgb'] = [LGBMClassifier(verbosity = 1), paramslgbc]\n",
    "    ##modelsc['knn'] = [KNeighborsClassifier(), paramsknnc]\n",
    "    ##modelsc['sgd'] = [SGDClassifier(verbose = 1), paramssgdc]\n",
    "    #modelsc['gbm'] = [XGBClassifier(seed = 23, verbosity = 1), paramsxgbc]\n",
    "    #modelsc['logit'] = [LogisticRegression(solver = 'sag'), paramslogitc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 13.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_time = 2020-01-21 10:10:43.948600...\n"
     ]
    }
   ],
   "source": [
    "    for key, value in modelsc.items():\n",
    "        \n",
    "        start_timec = timeit.default_timer()\n",
    "                \n",
    "        scorer = ['roc_auc', 'accuracy', 'f1', 'jaccard', 'precision', 'recall']\n",
    "                       \n",
    "        numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (0,1)))\n",
    "        categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "        preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "        \n",
    "        all_pipe = make_pipeline(preprocessor, value[0])\n",
    "                    \n",
    "        grid_search = GridSearchCV(all_pipe, value[1], cv=5, verbose=1, refit = 'roc_auc', scoring = scorer, return_train_score = True, n_jobs = -1)\n",
    "        #scoring option is for defining multiple scorers, otherwise null is OK\n",
    "        #refit must be chosen if multi scorers is selected. But best_score_, best_params_ etc will give only the result of that metric, cant get multi scores\n",
    "        #here REFIT option tells you which metric do you want to consider for best_params_ calculation(according to which metric)\n",
    "     \n",
    "        grid_search.fit(df_x, df_y_class.values.ravel())\n",
    "        \n",
    "        #from the resulting parameters dictionary, we choose the index of best_param_ set. This index will help us the get best param value from subresults\n",
    "        #metric results are in arraf form, in each array there is a list of results corresponding to the all parameter combinations. \n",
    "        ind = grid_search.best_index_      \n",
    "        \n",
    "        print(\"model = {}\".format(key), file = text_file)\n",
    "        print(\"train_roc = {}, test_roc ={}\".format(grid_search.cv_results_['mean_train_roc_auc'][ind], grid_search.cv_results_['mean_test_roc_auc'][ind]), file = text_file)\n",
    "        print(\"train_acc = {}, test_acc ={}\".format(grid_search.cv_results_['mean_train_accuracy'][ind], grid_search.cv_results_['mean_test_accuracy'][ind]), file = text_file)\n",
    "        print(\"train_f1 = {}, test_f1 = {}\".format(grid_search.cv_results_['mean_train_f1'][ind], grid_search.cv_results_['mean_test_f1'][ind]), file = text_file)\n",
    "        print(\"train_jaccard = {}, test_jaccard = {}\".format(grid_search.cv_results_['mean_train_jaccard'][ind], grid_search.cv_results_['mean_test_jaccard'][ind]), file = text_file)\n",
    "        print(\"train_precision = {}, test_precision = {}\".format(grid_search.cv_results_['mean_train_precision'][ind], grid_search.cv_results_['mean_test_precision'][ind]), file = text_file)\n",
    "        print(\"train_recall = {}, test_recall = {}\".format(grid_search.cv_results_['mean_train_recall'][ind], grid_search.cv_results_['mean_test_recall'][ind]), file = text_file)\n",
    "        print(\"avg_fit_time = {}\".format(grid_search.cv_results_['mean_fit_time'][ind]), file = text_file)\n",
    "        #we are getting best_params_ from grid_search object still, it is valid\n",
    "        print(\"best_params = {}\".format(grid_search.best_params_), file = text_file)\n",
    "        print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_timec)/60), file = text_file)\n",
    "        print(\"run_start = {}...\".format(start_time), file = text_file)\n",
    "        print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "        print(\"current_time = {}...\".format(datetime.now()))\n",
    "        print(\"-----------------------------------\", file = text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code is a hyperparameter search code with a CV. All included models are run with hyperparameter search and at the end all results are written inside a text file as a summary. Written results are regarding the fold with best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Lasso, Ridge, ElasticNet, HuberRegressor, Lars, RANSACRegressor, PassiveAggressiveRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regress == 1:\n",
    "\n",
    "    models = dict()\n",
    "    \n",
    "    paramsgbm = {\n",
    "            'n_estimator': np.linspace(100,500,2,endpoint = True),\n",
    "            'max_depth': np.linspace(1,9,3, endpoint=True, dtype = int),\n",
    "            #'min_child_weight':np.linspace(1, 9, 3, endpoint=True),\n",
    "            'colsample_bytree': np.linspace(0.1,0.9,3,endpoint =True)\n",
    "            }\n",
    "    \n",
    "    paramslr = {\n",
    "                }\n",
    "    \n",
    "    paramslas = {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1],\n",
    "            'max_iter': np.linspace(0, 3000, 3, endpoint = True),\n",
    "            'tol': [0.0001, 0.001, 0.01]\n",
    "                }\n",
    "    \n",
    "    paramsrid = {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1],\n",
    "            'max_iter': np.linspace(0, 3000, 3, endpoint = True),\n",
    "            'tol': [0.0001, 0.001, 0.01]\n",
    "                }\n",
    "    \n",
    "    paramsela = {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1],\n",
    "            'max_iter': np.linspace(0, 3000, 3, endpoint = True),\n",
    "            'tol': [0.0001, 0.001, 0.01],\n",
    "            'l1_ratio': np.linspace(0.25,0.75, 3, endpoint = True)\n",
    "            }\n",
    "    \n",
    "    paramssgd = {\n",
    "            'penalty':['none', 'l1','l2','elasticnet'],\n",
    "            'max_iter': np.linspace(0.1, 3000, 3 ,endpoint = True),\n",
    "            'alpha': [0.001, 0.01, 0.1, 1],\n",
    "            'n_iter_no_change':np.linspace(5, 10, 2, endpoint=True),\n",
    "            'l1_ratio': np.linspace(0.25,0.75,3,endpoint = True)\n",
    "            }\n",
    "    \n",
    "    paramsrf = {\n",
    "            'max_depth':[#5, 9, \n",
    "                         18, 32],\n",
    "            'n_estimators': [#10, 50, \n",
    "                             100, 200],\n",
    "            'min_samples_split': [0.1, 1.0, 2],\n",
    "            'min_samples_leaf': [0.1, 0.5, 1]\n",
    "            }\n",
    "    \n",
    "    paramslgb = {\n",
    "            'max_depth':[5, 9, 18, 32],\n",
    "            'learning_rate': [0.001, 0.01, 0.1],\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "            'num_leaves': np.linspace(11,51,3,endpoint = True, dtype = int)\n",
    "            }\n",
    "        \n",
    "    paramslgb2 = {\n",
    "            'max_depth':[9, 18, 32, 64],\n",
    "            'learning_rate': [0.001, 0.01, 0.1],\n",
    "            'n_estimators': [100, 200, 400, 600],\n",
    "            'num_leaves': np.linspace(11,71,4,endpoint = True, dtype = int)\n",
    "            }\n",
    "    \n",
    "    #models['gbm'] = [XGBRegressor(objective = 'reg:squarederror', booster = 'gbtree', seed = 23, learning_rate = 0.01), paramsgbm]\n",
    "    #models['lr'] = [LinearRegression(), paramslr]\n",
    "    #models['las'] = [Lasso(), paramslas]\n",
    "    #models['rid'] = [Ridge(), paramsrid]\n",
    "    #models['ela'] = [ElasticNet(), paramsela]\n",
    "    #models['huber'] = HuberRegressor()\n",
    "    #models['lars'] = Lars()\n",
    "    #models['passive'] = PassiveAggressiveRegressor()\n",
    "    #models['ransac'] = RANSACRegressor()\n",
    "    #models['sgd'] = [SGDRegressor(), paramssgd]\n",
    "    #models['rf'] = [RandomForestRegressor(), paramsrf]\n",
    "    models['lgb'] = [LGBMRegressor(), paramslgb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code tries all active models defined above with their related parameter spaces. \n",
    "\n",
    "First part of the if structure is for a single run scheme. \n",
    "Second part is for a nestedCV scheme where total run size is outerCV*innerCV*param_space . \n",
    "At the last part of if structure, there is a classical gridsearch scheme with a CV. Total run size of this part is CV*param_space .\n",
    "\n",
    "All results are written inside a txt file afterwards for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_time = 2020-01-21 10:31:35.788602...\n"
     ]
    }
   ],
   "source": [
    "    for key, value in models.items():\n",
    "        \n",
    "        if hypertest != 1:\n",
    "            \n",
    "            start_time = timeit.default_timer()\n",
    "            \n",
    "            numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (-1,1)))\n",
    "            categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "            #categoric_pipe = make_pipeline(ce.HashingEncoder())\n",
    "            preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "            \n",
    "            all_pipe = make_pipeline(preprocessor, value[0])\n",
    "            \n",
    "            cv = KFold(n_splits = 5, random_state = 23, shuffle = True)\n",
    "            \n",
    "            scorer = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "            \n",
    "            scores = cross_validate(all_pipe, df_x, df_y.values.ravel(), scoring = scorer, cv = cv, return_train_score = True)\n",
    "            \n",
    "            print(\"model = {}\".format(key), file = text_file)\n",
    "            print(\"train_mse = {}, test_mse ={}\".format(np.mean(scores['train_neg_mean_squared_error']), np.mean(scores['test_neg_mean_squared_error'])), file = text_file)\n",
    "            print(\"train_mae = {}, test_mae ={}\".format(np.mean(scores['train_neg_mean_absolute_error']), np.mean(scores['test_neg_mean_absolute_error'])), file = text_file)\n",
    "            print(\"train_r2 = {}, test_r2 = {}\".format(np.mean(scores['train_r2']), np.mean(scores['test_r2'])), file = text_file)\n",
    "            print(\"avg_fit_time = {}\".format(np.mean(scores['fit_time'])), file = text_file)\n",
    "            print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60), file = text_file)\n",
    "            print(\"...{}...\".format(start_time), file = text_file)\n",
    "            print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "            print(\"current_time = {}...\".format(datetime.now()))\n",
    "            print(\"-----------------------------------\", file = text_file)\n",
    "        \n",
    "        else:\n",
    "            if nestedcv == 1:\n",
    "                \n",
    "                start_time = timeit.default_timer()\n",
    "                 \n",
    "                numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (-1,1)))\n",
    "                categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "                preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "                \n",
    "                all_pipe = make_pipeline(preprocessor, value[0])\n",
    "                \n",
    "                grid_search = GridSearchCV(all_pipe, value[1], cv=3, verbose=0)\n",
    "                               \n",
    "                cv = KFold(n_splits = 5, random_state = 23, shuffle = False)\n",
    "                 \n",
    "                scorer = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "                #scores = cross_val_score(all_pipe, df_x, df_y.values.ravel(), scoring = 'neg_mean_squared_error', cv=cv)\n",
    "                #r2 = cross_val_score(all_pipe, df_x, df_y.values.ravel(), scoring = 'r2', cv=cv)\n",
    "                scores = cross_validate(grid_search, df_x, df_y.values.ravel(), scoring = scorer, cv = cv, return_train_score = True)\n",
    "                \n",
    "                #you have to fit grid_search object in order to get best_params_ from it. But here you also need a pipeline for preprocessing. \n",
    "                #fitting on pipeline object will give us the desired score. If it was not a pipeline, we would fit grid_search object after preprocessing\n",
    "                scores.fit(df_x, df_y.values.ravel())     \n",
    "                \n",
    "                print(\"model = {}\".format(key), file = text_file)\n",
    "                print(\"train_mse = {}, test_mse ={}\".format(np.mean(scores['train_neg_mean_squared_error']), np.mean(scores['test_neg_mean_squared_error'])), file = text_file)\n",
    "                print(\"train_mse = {}, test_mse ={}\".format(np.mean(scores['train_neg_mean_absolute_error']), np.mean(scores['test_neg_mean_absolute_error'])), file = text_file)\n",
    "                print(\"train_r2 = {}, test_r2 = {}\".format(np.mean(scores['train_r2']), np.mean(scores['test_r2'])), file = text_file)\n",
    "                print(\"avg_fit_time = {}\".format(np.mean(scores['fit_time'])), file = text_file)\n",
    "                #we ar getting best_params_ from grid_search object still, it is valid\n",
    "                print(\"best_params = {}\".format(grid_search.best_params_), file = text_file)\n",
    "                print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60), file = text_file)\n",
    "                print(\"...{}...\".format(start_time), file = text_file)\n",
    "                print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "                print(\"current_time = {}...\".format(datetime.now()))\n",
    "                print(\"-----------------------------------\", file = text_file)\n",
    "                \n",
    "                best_params = dict()\n",
    "                best_params[key] = grid_search.best_params_\n",
    "                \n",
    "                best_results = dict()\n",
    "                best_results[key] = [np.mean(scores['test_neg_mean_squared_error']), np.mean(scores['test_neg_mean_absolute_error']), np.mean(scores['test_r2'])]\n",
    "            \n",
    "            #this part is for only gridsearchcv. We can use CV results, as well as best parameters of the grid search from here. \n",
    "            else:\n",
    "                \n",
    "                start_time = timeit.default_timer()\n",
    "                \n",
    "                scorer = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "                \n",
    "                numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (0,1)))\n",
    "                categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "                preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "                \n",
    "                all_pipe = make_pipeline(preprocessor, value[0])\n",
    "                \n",
    "                #njobs = -1 uses 2 cores (all available) and 4 threads; consumes 100% of CPU. But it is faster absolutely. Njobs = 1 uses 1 core only, slower but consumes less CPU\n",
    "                grid_search = GridSearchCV(all_pipe, value[1], cv=5, verbose=1, refit = 'neg_mean_squared_error', scoring = scorer, return_train_score = True, n_jobs = -1)\n",
    "                #scoring option is for defining multiple scorers, otherwise null is OK\n",
    "                #refit must be chosen if multi scorers is selected. But best_score_, best_params_ etc will give only the result of that metric, cant get multi scores\n",
    "                #here REFIT option tells you which metric do you want to consider for best_params_ calculation(according to which metric)\n",
    "                                \n",
    "                grid_search.fit(df_x_regr, df_y_regr.values.ravel())\n",
    "                #all_pipe.fit(df_x, df_y.values.ravel())\n",
    "                \n",
    "                #from the resulting parameters dictionary, we choose the index of best_param_ set. This index will help us the get best param value from subresults\n",
    "                #metric results are in arraf form, in each array there is a list of results corresponding to the all parameter combinations. \n",
    "                ind = grid_search.best_index_      \n",
    "                \n",
    "                print(\"model = {}\".format(key), file = text_file)\n",
    "                print(\"train_mse = {}, test_mse ={}\".format(grid_search.cv_results_['mean_train_neg_mean_squared_error'][ind], grid_search.cv_results_['mean_test_neg_mean_squared_error'][ind]), file = text_file)\n",
    "                print(\"train_mae = {}, test_mae ={}\".format(grid_search.cv_results_['mean_train_neg_mean_absolute_error'][ind], grid_search.cv_results_['mean_test_neg_mean_absolute_error'][ind]), file = text_file)\n",
    "                print(\"train_r2 = {}, test_r2 = {}\".format(grid_search.cv_results_['mean_train_r2'][ind], grid_search.cv_results_['mean_test_r2'][ind]), file = text_file)\n",
    "                print(\"avg_fit_time = {}\".format(grid_search.cv_results_['mean_fit_time'][ind]), file = text_file)\n",
    "                #we are getting best_params_ from grid_search object still, it is valid\n",
    "                print(\"best_params = {}\".format(grid_search.best_params_), file = text_file)\n",
    "                print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60), file = text_file)\n",
    "                print(\"...{}...\".format(start_time), file = text_file)\n",
    "                print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "                print(\"current_time = {}...\".format(datetime.now()))\n",
    "                print(\"-----------------------------------\", file = text_file)\n",
    "                \n",
    "                best_params = dict()\n",
    "                best_params[key] = grid_search.best_params_\n",
    "                \n",
    "                best_results = dict()\n",
    "                best_results[key] = [grid_search.cv_results_['mean_test_neg_mean_squared_error'][ind], grid_search.cv_results_['mean_test_neg_mean_absolute_error'][ind], grid_search.cv_results_['mean_test_r2'][ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if neural_model == 1:\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.wrappers.scikit_learn import KerasRegressor\n",
    "    from keras.optimizers import RMSprop, SGD\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from keras.models import load_model\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    #these are inactive if cross validation will be used. Test set is applied several times for CV.\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(df_x, df_y.values.ravel(), test_size = 0.1, random_state = 23)\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(df_x, df_y.values.ravel(), test_size = 0.25)\n",
    "    \n",
    "    numeric_pipe = make_pipeline(Normalizer())\n",
    "    #numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (-1,1)))\n",
    "    #SPARSE MATRICES MAY BE MORE EFFICIENT BUT IT IS NOT VERY SUITABLE FOR KERAS MODELS\n",
    "    categoric_pipe = make_pipeline(OneHotEncoder(sparse = False, handle_unknown='ignore'))\n",
    "    preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "            \n",
    "    all_pipe2 = make_pipeline(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code is related to the neural network training. Preprocess part starts here. Then there is an inactive single run part below(Cross validation for neural networks is mostly ineffective because of huge datasets but in this case, my dataset is not huge and I could run a CV model easily) and also an active CV run part. NORMALIZATON, and ALSO SCALING AFTERWARDS, IS CRUCIAL FOR NNet preprocessing.!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\nx_train = all_pipe2.fit_transform(x_train)\\nx_test = all_pipe2.transform(x_test)\\n\\n#target_pipe = make_pipeline(Normalizer())\\n\\n#y_train = target_pipe.fit_transform(y_train.reshape(-1,1))\\n#y_test = target_pipe.transform(y_test.reshape(-1,1))\\n#reshape is required for normalization like transformations of 1D arrays(output values for example)\\n\\noptimizer = RMSprop(0.001)\\nsgd = SGD(lr=0.1, momentum=0.8,nesterov=False)\\n  \\nmodel = Sequential()\\nmodel.add(Dense(30, input_dim=x_train.shape[1], kernel_initializer=\\'normal\\', activation=\\'relu\\'))\\nmodel.add(Dense(10, activation=\\'relu\\'))\\nmodel.add(Dense(1, activation=\\'linear\\'))\\nmodel.summary()\\n\\nmodel.compile(loss=\\'mse\\', optimizer=\\'adam\\', metrics=[\\'mse\\',\\'mae\\'])\\n\\npatience = 100\\n#early stopping stops training after n unsuccesful runs. Model checkpoint saves that best model n epochs before termination.\\ncallbacks = [EarlyStopping(monitor=\\'val_mean_squared_error\\', patience=patience),\\n             ModelCheckpoint(filepath=\\'best_model.h5\\', monitor=\\'val_mean_squared_error\\', save_best_only=True)]\\n#callbacks = [EarlyStopping(monitor=\\'val_mean_squared_error\\', patience=patience)]   \\n\\nhistory = model.fit(x_train, y_train, epochs=10000, batch_size=30,  verbose=1, validation_split=0.2, callbacks = callbacks)\\n\\n#scores = model.evaluate(x_test, y_test, verbose=0)\\n#print(scores)\\n#print(model.metrics_names)\\n\\n   \\nplt.figure()\\nplt.xlabel(\\'Epoch\\')\\nplt.ylabel(\\'Mean_Square_Error\\')\\nplt.gca().set_xlim([100, np.max(history.epoch)])\\nplt.plot(history.epoch, history.history[\\'mean_squared_error\\'],label=\\'Train Error\\')\\nplt.plot(history.epoch, history.history[\\'val_mean_squared_error\\'],label = \\'Val Error\\')\\nplt.legend([\\'train\\', \\'validation\\'], loc=\\'upper left\\')\\nplt.show()\\n\\nprint(\"Average of last +-25 best epochs of best epoch:\")\\nprint(np.mean(history.history[\\'val_mean_squared_error\\'][(np.max(history.epoch)-patience-25):(np.max(history.epoch)-patience+25)]))\\nprint(\"............................\")\\n\\nsaved_model = load_model(\\'best_model.h5\\')\\n\\nloss, mse, mae = saved_model.evaluate(x_test, y_test, verbose=0)\\nprint(\"Test set MSE: {}\".format(mse))\\nprint(\"Test set MAE: {}\".format(mae))\\nprint(\"Test set LOSS: {}\".format(loss))\\nprint(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60))\\n\\n### IF you use a CROSS VALIDATION and your results vary so much, this means you need to increase epoch, increase learning rate\\n# or decrease batch size, or all. \\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    '''   \n",
    "    x_train = all_pipe2.fit_transform(x_train)\n",
    "    x_test = all_pipe2.transform(x_test)\n",
    "    \n",
    "    #target_pipe = make_pipeline(Normalizer())\n",
    "    \n",
    "    #y_train = target_pipe.fit_transform(y_train.reshape(-1,1))\n",
    "    #y_test = target_pipe.transform(y_test.reshape(-1,1))\n",
    "    #reshape is required for normalization like transformations of 1D arrays(output values for example)\n",
    "    \n",
    "    optimizer = RMSprop(0.001)\n",
    "    sgd = SGD(lr=0.1, momentum=0.8,nesterov=False)\n",
    "      \n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    \n",
    "    patience = 100\n",
    "    #early stopping stops training after n unsuccesful runs. Model checkpoint saves that best model n epochs before termination.\n",
    "    callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=patience),\n",
    "                 ModelCheckpoint(filepath='best_model.h5', monitor='val_mean_squared_error', save_best_only=True)]\n",
    "    #callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=patience)]   \n",
    "    \n",
    "    history = model.fit(x_train, y_train, epochs=10000, batch_size=30,  verbose=1, validation_split=0.2, callbacks = callbacks)\n",
    "    \n",
    "    #scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    #print(scores)\n",
    "    #print(model.metrics_names)\n",
    "\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean_Square_Error')\n",
    "    plt.gca().set_xlim([100, np.max(history.epoch)])\n",
    "    plt.plot(history.epoch, history.history['mean_squared_error'],label='Train Error')\n",
    "    plt.plot(history.epoch, history.history['val_mean_squared_error'],label = 'Val Error')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Average of last +-25 best epochs of best epoch:\")\n",
    "    print(np.mean(history.history['val_mean_squared_error'][(np.max(history.epoch)-patience-25):(np.max(history.epoch)-patience+25)]))\n",
    "    print(\"............................\")\n",
    "    \n",
    "    saved_model = load_model('best_model.h5')\n",
    "    \n",
    "    loss, mse, mae = saved_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"Test set MSE: {}\".format(mse))\n",
    "    print(\"Test set MAE: {}\".format(mae))\n",
    "    print(\"Test set LOSS: {}\".format(loss))\n",
    "    print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60))\n",
    "    \n",
    "    ### IF you use a CROSS VALIDATION and your results vary so much, this means you need to increase epoch, increase learning rate\n",
    "    # or decrease batch size, or all. \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    kfold = KFold(n_splits = 5, random_state = 23, shuffle = True)\n",
    "    \n",
    "    epochbestlist = []\n",
    "    epochbestvallist = []\n",
    "    epochavglist = []\n",
    "    epochvalavglist = []\n",
    "    testmselist = []\n",
    "    testmaelist = []\n",
    "    testlosslist= []\n",
    "    \n",
    "    ### IF you use a CROSS VALIDATION and your results vary so much, this means you need to increase epoch, increase learning rate\n",
    "    # or decrease batch size, or all. \n",
    "    for train_id, test_id in kfold.split(df_x_regr, df_y_regr.values.ravel()):\n",
    "    \n",
    "        patience = 50\n",
    "\n",
    "        cv_x = df_x_regr.iloc[train_id]\n",
    "        cv_x_test = df_x_regr.iloc[test_id]\n",
    "        #normally, if we pass column transformer pipeline into CROSS_VAL_SCORE directly etc, we also define DATAFRAMES with it. but here if we use df_x.values[train_id]\n",
    "        #it is not a dataframe anymore. and we cant pass it into a column transformer pipeline like that. So, I used a datafram filter not to lose the structure. \n",
    "        \n",
    "        cv_y = df_y_regr.values.ravel()[train_id]\n",
    "        cv_y_test = df_y_regr.values.ravel()[test_id]\n",
    "        \n",
    "        print(cv_x)\n",
    "        print(cv_y)\n",
    "        print(cv_x_test)\n",
    "        \n",
    "        optimizerr = RMSprop(0.001)\n",
    "        \n",
    "        cv_x_pre = all_pipe2.fit_transform(cv_x)\n",
    "        cv_x_pre_test = all_pipe2.transform(cv_x_test)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, input_dim=cv_x_pre.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=optimizerr, metrics=['mse','mae'])  \n",
    "        \n",
    "        callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=patience),\n",
    "                 ModelCheckpoint(filepath='best_model.h5', monitor='val_mean_squared_error', save_best_only=True)]  \n",
    "        \n",
    "        history = model.fit(cv_x_pre, cv_y, callbacks = callbacks, epochs = 10000, batch_size = 10, verbose = 1, validation_split = 0.20)\n",
    "        \n",
    "        epochbest = np.min(history.history['mean_squared_error'])\n",
    "        epochvalbest = np.min(history.history['val_mean_squared_error'])\n",
    "        \n",
    "        epochavg = np.mean(history.history['mean_squared_error'][(np.max(history.epoch)-patience-5):(np.max(history.epoch)-patience+5)])\n",
    "        epochvalavg = np.mean(history.history['val_mean_squared_error'][(np.max(history.epoch)-patience-5):(np.max(history.epoch)-patience+5)])\n",
    "        \n",
    "        saved_model = load_model('best_model.h5')\n",
    "        \n",
    "        loss, mse, mae = saved_model.evaluate(cv_x_pre_test, cv_y_test, verbose=0)\n",
    "        \n",
    "        print(\"Best epoch train MSE:\", file = text_file)\n",
    "        print(epochbest, file = text_file)\n",
    "        print(\"............................\", file = text_file)\n",
    "        \n",
    "        print(\"Best epoch validation MSE:\", file = text_file)\n",
    "        print(epochvalbest, file = text_file)\n",
    "        print(\"............................\", file = text_file)\n",
    "        \n",
    "        print(\"Average MSE of last +-25 best epochs of best epoch (train):\", file = text_file)\n",
    "        print(epochavg, file = text_file)\n",
    "        print(\"............................\", file = text_file)\n",
    "        \n",
    "        print(\"Average MSE of last +-25 best epochs of best epoch (validation):\", file = text_file)\n",
    "        print(epochvalavg,file = text_file)\n",
    "        print(\"............................\", file = text_file)\n",
    "\n",
    "        print(\"Test set MSE: {}\".format(mse), file = text_file)\n",
    "        print(\"Test set MAE: {}\".format(mae), file = text_file)\n",
    "        print(\"Test set LOSS: {}\".format(loss), file = text_file)\n",
    "                \n",
    "        epochbestlist.append(epochbest)\n",
    "        epochbestvallist.append(epochvalbest)\n",
    "        epochavglist.append(epochavg)\n",
    "        epochvalavglist.append(epochvalavg)\n",
    "        testmselist.append(mse)\n",
    "        testmaelist.append(mae)\n",
    "        testlosslist.append(loss)\n",
    "        \n",
    "        \n",
    "    print('Mean of epoch best MSE:',np.mean(epochbestlist), file = text_file)\n",
    "    print('Mean of epoch best validation MSE:',np.mean(epochbestvallist), file = text_file)\n",
    "    print('Mean of epoch avg MSE:',np.mean(epochavglist), file = text_file)\n",
    "    print('Mean of epoch avg validation MSE:',np.mean(epochvalavglist), file = text_file)\n",
    "    print('Mean test MSE:',np.mean(testmselist), file = text_file)\n",
    "    print('Mean test MAE:',np.mean(testmaelist), file = text_file)\n",
    "    print('Mean test LOSS:',np.mean(testlosslist), file = text_file)\n",
    "    print(\"current_time = {}...\".format(datetime.now()), file = text_file)\n",
    "    print(\"current_time = {}...\".format(datetime.now()))\n",
    "    print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60), file = text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the neural model above with CV, each fold score is stored inside lists. Inside each fold, a patience value is determined and fold automatically stops after P not-improved epochs from best epoch. And the best epoch scores are stored for this fold. \n",
    "\n",
    "After each folds are completed, mean of these best scores are summarized at the end of process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is about finalizing the best model with full dataset and making predictions on it.\n",
    "\n",
    "After choosing the best hyperparameters of the best model at previous section, these hyperparameters are written into this part. Then we are applying a preprocessing part, again, on whole data. Due to the caution of data leakage preprocessing is applied on train data only. In here, train data is whole data. So we have to preprocess them together. \n",
    "\n",
    "As it is a 2 staged model, first one is a classification model and the next one is a regression model, and there are different input datasets for each one, I constructed 2 different pipelines for each preprocessing section. \n",
    "\n",
    "Then code iterates over each company_code, from, to combination. It retrieves data from prediction dataset for a combination C. It gets predictions rowwise. 1st row is applied on classification first, if it is not 0, then it is applied on regression algorithm and gets a prediction. This prediction value (after a reverse transformation is applied, if it is transformed before) is written into available spots for next month values of other rows. (For example it is written into M-1 column for next month's row and it is also written into M-2 column for (i+2)th row etc. With the help of this approach, all empty places inside future months are filled with prediction values. \n",
    "\n",
    "Each combination inside prediction dataset is predicted like this, then they will appear as a single row at the end with predicted values for next N months appearing as columns inside output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if finalize == 1:\n",
    "    \n",
    "    #rfc = RandomForestClassifier(max_depth = 14.28, min_samples_leaf = 0.1, min_samples_split = 0.1, n_estimators = 200)\n",
    "    lgbc = LGBMClassifier(learning_rate = 0.1, max_depth = 32, n_estimators = 200, num_leaves = 51)\n",
    "    #sgdc = SGDClassifier(alpha = 0.0001, max_iter = 100, n_iter_no_change = 11, penalty = 'elasticnet')\n",
    "    \n",
    "    lgbr = LGBMRegressor(learning_rate= 0.1, max_depth= 9, n_estimators= 200, num_leaves= 31)\n",
    "\n",
    "    numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (0,1)))\n",
    "    categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "    #categoric_pipe = make_pipeline(ce.HashingEncoder())\n",
    "    preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "    \n",
    "    regr_pipe_final = make_pipeline(preprocessor, lgbr)\n",
    "    \n",
    "    regr_pipe_final.fit(df_x_regr, df_y_regr.values.ravel())\n",
    "    #regr_pipe_final.fit(df_x, df_y.values.ravel())\n",
    "    \n",
    "    class_pipe_final = make_pipeline(preprocessor, lgbc)\n",
    "    \n",
    "    class_pipe_final.fit(df_x, df_y_class.values.ravel())\n",
    "    \n",
    "    #unique_everseen function gets a set from list, removing duplicates, but it also obeys the ordering. SET function do not take orders into account.\n",
    "    print(list(mi.unique_everseen(pred_set['code'] + pred_set['from'] + pred_set['to'])))\n",
    "    \n",
    "    pred_final = pd.DataFrame()\n",
    "    prob_list = []\n",
    "    for key in list(mi.unique_everseen(pred_set['code'] + pred_set['from'] + pred_set['to'])):\n",
    "    #for key in ['0000124707DETR']:\n",
    "    #for key in list(mi.unique_everseen(pred_set['code'] + pred_set['comb'])):\n",
    "        \n",
    "        pred_val_list = []\n",
    "        \n",
    "        pred_subset = pred_set[(pred_set['code'] + pred_set['from'] + pred_set['to']) == key]\n",
    "        #pred_subset = pred_set[(pred_set['code'] + pred_set['comb']) == key]\n",
    "        \n",
    "        lag = 0          \n",
    "        #print(pred_subset)\n",
    "        for i in range(0,predmonths):\n",
    "                    \n",
    "            #iloc[i,:] returns series with columns as indexes for single row output but this returns dataframe with proper indexing\n",
    "            pred = pred_subset.iloc[[i],:]\n",
    "            #print(pred)\n",
    "            #moving averages should be calculated dynamically before making a prediction for a row\n",
    "            for key2, value in movingdict.items():               \n",
    "                pred.loc[:,'moving'+str(key2)] = (pred[value].sum(axis=1))/key2\n",
    "            #print(pred)\n",
    "            pred.drop(columns = pred_drop, axis = 1, inplace = True)\n",
    "            #print(pred)\n",
    "            class_val = class_pipe_final.predict(pred)\n",
    "            \n",
    "            class_prob = class_pipe_final.predict_proba(pred)\n",
    "            #print(class_val)\n",
    "            regr_val = regr_pipe_final.predict(pred)\n",
    "            #print(regr_val)\n",
    "            if class_val[0] == 0:\n",
    "                prob_list.append(class_prob[0][0])\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            for j in range (i+1, predmonths+1):\n",
    "                #using directly this one do not WORK. FILTERED dataframe is assumed as COPY, and it is not owerwritten on it, i guess. I couldn't find\n",
    "                # I was directly trying to replace on pred_set, but it didn't work. I created pred_subset and pred_final eventually.\n",
    "                #print(pred_set.loc[(pred_set['from'] + pred_set['to']) == key, :].iloc[j, pred_set.columns.get_loc(str('M-'+str(j)))])\n",
    "                #it may still raise the same error but it is a kind of bug and, if you used iloc do not think about that. It is written on documentation also\n",
    "                \n",
    "                if (class_val[0] == 0):\n",
    "                #if (class_prob[0][0] >= 0.40):\n",
    "                    \n",
    "                    pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = class_val[0]\n",
    "                    \n",
    "                else:\n",
    "                    if transform == 1:\n",
    "                        pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = np.expm1(regr_val[0])\n",
    "                    else:\n",
    "                        pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = regr_val[0]\n",
    "            lag = lag + 1\n",
    "        #print(pred_subset)\n",
    "        \n",
    "        #last moving average calculation for a combination, which will appear on final output.\n",
    "        for key3, value in movingdict.items():               \n",
    "            pred_subset.loc[:,'moving'+str(key3)] = (pred_subset[value].sum(axis=1))/key3\n",
    "            \n",
    "        #print(pred_subset)\n",
    "        pred_final = pred_final.append(pred_subset)\n",
    "\n",
    "    pred_final = pd.merge(pred_final, pred_unique, left_index=True, right_index = True)\n",
    "    pred_final = pred_final[pred_final['date'] == pred_final['date'].max()]\n",
    "        \n",
    "    #print_excel = pred_final.to_excel(r'C:\\Users\\ali.kilinc\\Desktop\\Genel8MHalfrWtNcyc.xlsx', index = None, header = True, sheet_name = 'FullTable')\n",
    "    #print_excel = pred_final.to_excel(r'C:\\Users\\ali.kilinc\\Desktop\\Last12CompWtNCyc10comb.xlsx', index = None, header = True, sheet_name = 'FullTable')\n",
    "    #print_excel = pred_final.to_excel(r'C:\\Users\\ali.kilinc\\Desktop\\FinalForecastClassAddedSpot.xlsx', index = None, header = True, sheet_name = 'FullTable')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is for finalization of Neural Model. If we decided to run a neural model and get the results from that, this part can be activated and best parameters should be written inside proper places before running the code.\n",
    "\n",
    "It will again run on whole dataset and there are also proper preprocessing pipelines because of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if neural_model_final == 1:\n",
    "        \n",
    "    numeric_pipe = make_pipeline(Normalizer())\n",
    "    #numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (-1,1)))\n",
    "    categoric_pipe = make_pipeline(OneHotEncoder(sparse = False, handle_unknown='ignore'))\n",
    "    preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "    all_pipe3 = make_pipeline(preprocessor)\n",
    "        \n",
    "    df_x_n = all_pipe3.fit_transform(df_x)\n",
    "    \n",
    "    optimizerr = RMSprop(0.001)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=df_x_n.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer=optimizerr, metrics=['mse','mae'])  \n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=patience),\n",
    "             ModelCheckpoint(filepath='best_model_final.h5', monitor='val_mean_squared_error', save_best_only=True)] \n",
    "    \n",
    "    history = model.fit(df_x_n, df_y, callbacks = callbacks, epochs = 10000, batch_size = 20, verbose = 1, validation_split = 0.2)\n",
    "    \n",
    "    saved_model_final = load_model('best_model_final.h5')\n",
    "\n",
    "    print(pred_set.head(10))\n",
    "    \n",
    "    #unique_everseen function gets a set from list, removing duplicates, but it also obeys the ordering. SET function do not take orders into account.\n",
    "    print(list(mi.unique_everseen(pred_set['code'] + pred_set['from'] + pred_set['to'])))\n",
    "    \n",
    "    pred_final = pd.DataFrame()\n",
    "    \n",
    "    for key in list(mi.unique_everseen(pred_set['code'] + pred_set['from'] + pred_set['to'])):\n",
    "        \n",
    "        pred_val_list = []\n",
    "        \n",
    "        pred_subset = pred_set[(pred_set['code'] + pred_set['from'] + pred_set['to']) == key]\n",
    "        \n",
    "        lag = 0\n",
    "            \n",
    "        for i in range(0,predmonths):\n",
    "                \n",
    "            pred = pred_subset.iloc[[i],:]\n",
    "\n",
    "            for key2, value in movingdict.items():               \n",
    "                pred.loc[:,'moving'+str(key2)] = (pred[value].sum(axis=1))/key2\n",
    "\n",
    "            pred.drop(columns = pred_drop, axis = 1, inplace = True)\n",
    "    \n",
    "            pred_val = saved_model_final.predict(all_pipe3.transform(pred))\n",
    "                \n",
    "            for j in range (i+1, predmonths+1):\n",
    "                    \n",
    "                if transform == 1:\n",
    "                    pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = np.expm1(pred_val[0])\n",
    "                else:\n",
    "                    pred_subset.iloc[j, pred_subset.columns.get_loc(str('M-'+str(j))) + lag] = pred_val[0]\n",
    "                        \n",
    "            lag = lag + 1\n",
    "        \n",
    "        for key3, value in movingdict.items():               \n",
    "            pred_subset.loc[:,'moving'+str(key3)] = (pred_subset[value].sum(axis=1))/key3\n",
    "        \n",
    "        pred_final = pred_final.append(pred_subset)\n",
    "       \n",
    "    pred_final = pd.merge(pred_final, pred_unique, left_index=True, right_index = True)\n",
    "    \n",
    "    pred_final = pred_final[pred_final['date'] == pred_final['date'].max()]\n",
    "        \n",
    "    print_excel = pred_final.to_excel(r'C:\\Users\\ali.kilinc\\Desktop\\Tahminleme\\FinalForecastNN4.xlsx', index = None, header = True, sheet_name = 'FullTable')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
